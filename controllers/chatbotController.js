const { codeLlama: importedCodeLlama } = require("../codeLlama");
const { llamacpp, streamText } = require("modelfusion");
const axios = require("axios");
const { exec } = require("child_process");

function getRunningPort() {
    return process.env.PORT || 3000;  // Change to your default port
}  

/** 
 * Validate if the given port number is within the valid range.
 */
function isValidPort(port) {
    const portNumber = parseInt(port, 10);
    return portNumber >= 1 && portNumber <= 65535;
}

/**
 * Logs incoming requests with method and URL.
 */
function logRequest(req, additionalData = {}) {
    console.log(`[${new Date().toISOString()}] Request received`);
    console.log(`Method: ${req.method}, URL: ${req.originalUrl}`);
    if (Object.keys(additionalData).length > 0) {
        console.log("Additional Data:", additionalData);
    }
}

/**
 * Determines if the given task is related to programming.
 */
function isProgrammingTask(task) {
    const programmingKeywords = ["code", "programming", "development", "script", "algorithm"];
    return programmingKeywords.some(keyword => task.toLowerCase().includes(keyword));
}

// Default AI model server port
const DEFAULT_PORT = process.env.AI_SERVER_PORT || 3000;

/**
 * Fetch AI response from the local AI server.
 */
async function fetchAIResponseFromServer(prompt, language, port = DEFAULT_PORT) {
    const url = `http://127.0.0.1:${port}/completion`;

    try {
        const response = await axios.post(url, { prompt, language }, { timeout: 60000 });
        return response.data.response || "‚ö†Ô∏è No response from AI server.";
    } catch (error) {
        console.error("‚ùå AI server error:", error.message);
        return "Error occurred while contacting AI model server.";
    }
}

/**
 * Main API handler to process AI requests.
 */
let requestCount = 0;

exports.getAIResponse = async (req, res) => {
    requestCount++;
    console.log(`Request count: ${requestCount}, Request body:`, req.body);

    if (requestCount > 10) {  // Arbitrary threshold to catch loops
        console.error("‚ö†Ô∏è Possible loop detected, stopping execution.");
        return res.status(500).json({ success: false, error: "Loop detected, stopping execution." });
    }

    try {
        let { prompt, instruction, language, port } = req.body;

        // Use instruction if prompt is missing
        prompt = prompt || instruction;
        port = Number(port) || DEFAULT_PORT;

        // Validate input
        if (!prompt || typeof prompt !== "string" || !prompt.trim()) {
            return res.status(400).json({ success: false, error: "Prompt is required." });
        }
        if (!isValidPort(port)) {
            return res.status(400).json({ success: false, error: "Invalid port number." });
        }

        logRequest(req, { prompt, language, port });
        console.log(`üîÑ Processing AI request on port ${port}...`);

        const isCodeTask = isProgrammingTask(prompt);
        let response;

        if (isCodeTask) {
            language = language || "python";
            console.log(`üñ•Ô∏è Using CodeLlama for ${language} task.`);
            response = await codeLlama(prompt, language, port);
        } else {
            console.log("üí¨ Using GeneralLlama for chatbot request.");
            response = await generalLlama(prompt);
        }

        if (!response || typeof response !== "string") {
            throw new Error("Received an empty or invalid response from AI model.");
        }

        console.log("‚úÖ AI Response successfully generated.");

        return res.status(200).json({
            success: true,
            response,
            metadata: {
                model: isCodeTask ? "CodeLlama" : "GeneralLlama",
                language: language || "N/A",
                processedAt: new Date().toISOString(),
            },
        });
    } catch (error) {
        console.error("üö® AI Response Error:", error);

        return res.status(500).json({
            success: false,
            error: "Internal server error. Please try again later.",
            details: error.message,
            timestamp: new Date().toISOString(),
        });
    } 
};
async function generalLlama(prompt) {
    const url = "http://localhost:3000/completion"; // Correct endpoint

    const payload = {
        prompt,
        temperature: 0.7,
        max_tokens: 200000000
    };

    const headers = { "Content-Type": "application/json" };

    console.log(`üîÑ Sending request to GeneralLlama at ${url}`);
    console.log("üì§ Payload:", JSON.stringify(payload, null, 2));

    try {
        const response = await axios.post(url, payload, { timeout: 120000, headers });

        console.log("‚úÖ GeneralLlama Response:", response.data);

        // üî• FIX: Ensure the response is a string
        let aiResponse = response.data.content || response.data.response;

        if (!aiResponse) {
            throw new Error("Empty or unexpected response from GeneralLlama.");
        }

        if (typeof aiResponse !== "string") {
            aiResponse = JSON.stringify(aiResponse); // Convert to string if it's an object
        }

        return aiResponse.trim();
    } catch (error) {
        console.error("‚ùå GeneralLlama request error:", error.message);

        if (error.response) {
            console.error("‚ùå Server Response:", error.response.data);
        }

        return `‚ö†Ô∏è AI Error: ${error.message}`;
    }
}



/**
 * Handles AI-generated code responses using CodeLlama.
 */
async function codeLlama(instruction, language, port) {
    const llamaSystemPrompt = `You are an AI assistant for programming tasks. Generate code in the specified language.`;
    const api = llamacpp.Api({ baseUrl: `http://localhost:${port}` });

    console.log(`üîÑ Sending request to CodeLlama (Port: ${port})`);
    console.log(`üìù Instruction: "${instruction}" | Language: ${language}`);

    try {
        const timeout = 15000000; // 15s timeout
        const controller = new AbortController();
        const timeoutId = setTimeout(() => {
            console.error("‚è≥ Request timeout: AI server took too long to respond.");
            controller.abort();
        }, timeout);

        const textStream = await streamText({
            signal: controller.signal,
            model: llamacpp
                .CompletionTextGenerator({ api, temperature: 0, stopSequences: ["\n```"] })
                .withInstructionPrompt(),
            prompt: {
                system: llamaSystemPrompt,
                instruction,
                responsePrefix: `Here is the program in ${language}:\n\`\`\`${language}\n`,
            },
        });

        let response = "";
        for await (const textPart of textStream) {
            process.stdout.write(textPart); // Stream response live
            response += textPart;
        }

        clearTimeout(timeoutId);

        if (!response.trim()) {
            throw new Error("Received empty response from CodeLlama.");
        }

        console.log("\n‚úÖ Response completed.");
        return response;
    } catch (error) {
        clearTimeout(timeoutId);
        console.error("‚ùå CodeLlama error:", error.message);

        return `‚ö†Ô∏è AI Error: ${error.message}. Check if AI server is running on port ${port}.`;
    }
}
 


/**
 * Summarizes text using the AI model.
 * @param {string} text - The text to summarize.
 * @param {number} port - The AI server port.
 * @returns {Promise<string>} - AI-generated summary.
 */
exports.summarizeText = async (text, port) => {
    exports.log("INFO", `üìú Summarizing text (Length: ${text.length} chars)`);
    return await exports.fetchAIResponseFromServer(`Summarize: ${text}`, "English", port);
};

/**
 * Analyzes text sentiment using the AI model.
 * @param {string} text - The text to analyze.
 * @param {number} port - The AI server port.
 * @returns {Promise<string>} - Sentiment analysis result.
 */
exports.analyzeSentiment = async (text, port) => {
    exports.log("INFO", `üßê Analyzing sentiment of text`);
    return await exports.fetchAIResponseFromServer(`Analyze sentiment: ${text}`, "English", port);
};

/**
 * Provides chatbot functionality by interacting with AI.
 * @param {string} userMessage - The user's message.
 * @param {number} port - The AI server port.
 * @returns {Promise<string>} - AI-generated chatbot response.
 */
exports.chatbot = async (userMessage, port) => {
    exports.log("INFO", `üí¨ Chatbot processing message: "${userMessage}"`);
    return await exports.fetchAIResponseFromServer(userMessage, "English", port);
};

/**
 * Refactors code for better readability and performance.
 * @param {string} code - The source code to refactor.
 * @param {string} language - Programming language.
 * @param {number} port - The AI server port.
 * @returns {Promise<string>} - Refactored code.
 */
exports.refactorCode = async (code, language, port = 4000) => {
    exports.log("INFO", `üîÑ Refactoring ${language} code...`);

    const llamaSystemPrompt = `You are an AI assistant specializing in code refactoring. Improve the given code while maintaining its functionality.`;

    const validPort = Number.isNaN(Number(port)) ? 4000 : port;
    const api = new llamacpp.Api({
        baseUrl: `http://localhost:${validPort}`,
    });

    const timeout = 10000; // Increased timeout to 10 seconds
    const controller = new AbortController();
    let timeoutId;

    try {
        timeoutId = setTimeout(() => {
            controller.abort();
            exports.log("ERROR", "‚è≥ Request timed out.");
        }, timeout);

        // Ensure 'code' is a valid string
        if (typeof code !== 'string') {
            if (typeof code === 'object' && code !== null) {
                if (code.body?.code) {
                    // Extract `code` from request body if present
                    code = code.body.code;
                } else {
                    exports.log("ERROR", "‚ùå Invalid 'code' parameter. Expected a string or valid object.");
                    return "Error: Invalid code input.";
                }
            } else {
                exports.log("ERROR", "‚ùå Code must be a string.");
                return "Error: Code must be a string.";
            }
        }

        // Log sanitized input
        exports.log("INFO", `Sending code to AI server:\n${code}`);
        exports.log("INFO", `Language: ${language}`);

        // Stream AI-generated refactored code
        const textStream = await streamText({
            signal: controller.signal,
            model: new llamacpp.CompletionTextGenerator({
                api,
                temperature: 0,
                stopSequences: ["\n"],
            }),
            prompt: {
                system: llamaSystemPrompt,
                instruction: `Please refactor the following ${language} code for better readability and efficiency:\n\`\`\`${language}\n${code}\n\`\`\``,
                responsePrefix: "Refactored code:\n",
            },
        });

        let response = "";
        for await (const textPart of textStream) {
            response += textPart;
        }

        clearTimeout(timeoutId); // Clear timeout after success
        exports.log("INFO", "‚úÖ Response completed.");
        return response.trim() || "No refactored code returned.";

    } catch (error) {
        clearTimeout(timeoutId); // Clear timeout in case of error
        
        // Log detailed error
        exports.log("ERROR", `‚ùå Refactoring error: ${error.message}`);

        if (error.response) {
            exports.log("ERROR", `API response: ${JSON.stringify(error.response)}`);
        } else if (error.stack) {
            exports.log("ERROR", `Stack trace: ${error.stack}`);
        }

        return `An error occurred while refactoring the code: ${error.message}`;
    }
};

/**
 * Optimizes code for better performance.
 * @param {string} code - The source code to optimize.
 * @param {string} language - Programming language.
 * @param {number} port - The AI server port.
 * @returns {Promise<string>} - Optimized code.
 */
exports.optimizeCode = async (code, language, port = 4000) => {
    exports.log("INFO", `üöÄ Optimizing ${language} code...`);

    if (!port || isNaN(port)) {
        exports.log("ERROR", `‚ùå Invalid port: ${port}. Using default port 4000.`);
        port = 4000; // Default port if invalid
    }

    const llamaSystemPrompt = `You are an AI assistant specializing in code optimization. Improve the given code by reducing time complexity or memory usage.`;

    const api = new llamacpp.Api({
        baseUrl: `http://localhost:${port}`,
    });

    let timeoutId = null; // Declare timeoutId outside try block

    try {
        const timeout = 5000;
        const controller = new AbortController();
        timeoutId = setTimeout(() => controller.abort(), timeout); // Assign timeoutId

        const textStream = await streamText({
            signal: controller.signal,
            model: new llamacpp.CompletionTextGenerator({
                api,
                temperature: 0,
                stopSequences: ["\n"],
            }),
            prompt: {
                system: llamaSystemPrompt,
                instruction: `Optimize the following ${language} code for performance:\n\`\`\`${language}\n${code}\n\`\`\``,
                responsePrefix: "Optimized code:\n",
            },
        });

        let response = "";
        for await (const textPart of textStream) {
            response += textPart;
        }

        clearTimeout(timeoutId);
        return response.trim() || "No optimized code returned.";

    } catch (error) {
        if (timeoutId) clearTimeout(timeoutId); // Only clear timeout if it was set
        exports.log("ERROR", `‚ùå Optimization error: ${error.message}`);
        return "An error occurred while optimizing the code.";
    }
};

/**
 * Main API handler to process AI requests.
 * @param {Object} req - Express request object.
 * @param {Object} res - Express response object.
 */
exports.getAIResponse = async (req, res) => {
    console.log("Incoming Request Body:", req.body);
    let { prompt, instruction, language, port } = req.body;

    prompt = prompt || instruction;
    port = Number(port) || getRunningPort();

    if (!prompt || typeof prompt !== "string" || !prompt.trim()) {
        return res.status(400).json({ error: 'Prompt is required and must be a non-empty string' });
    }

    if (!isValidPort(port)) {
        return res.status(400).json({ error: 'Invalid port number' });
    }

    exports.log("INFO", `Request received with prompt: "${prompt}"`);

    try {
        let response;
        if (isProgrammingTask(prompt)) {
            language = language || "python";
            exports.log("INFO", `üñ•Ô∏è Detected programming request. Using CodeLlama for ${language}.`);
            response = await importedCodeLlama(prompt, language, port);
        } else {
            exports.log("INFO", "üí¨ General chatbot request detected. Using GeneralLlama.");
            response = await generalLlama(prompt, port);
        }

        if (!response) {
            throw new Error('Empty response received from AI model');
        }

        return res.status(200).json({ response });
    } catch (error) {
        exports.log("ERROR", `üö® AI Response Error: ${error.message}`);
        return res.status(500).json({ error: 'Internal server error', details: error.message });
    }
};

/**
 * Helper function for logging.
 * @param {string} level - Log level (INFO, ERROR, etc.).
 * @param {string} message - Log message.
 */
exports.log = (level, message) => {
    console.log(`[${level}] ${message}`);
};

/**
 * Suggests an algorithm for a given problem.
 * @param {string} problemDescription - Description of the problem.
 * @param {number} port - The AI server port.
 * @returns {Promise<string>} - Suggested algorithm.
 */
exports.suggestAlgorithm = async (problemDescription, port = 4000) => {
    const llamaSystemPrompt = `You are an AI assistant specializing in algorithm design. 
    Given a problem description, suggest an appropriate algorithm or approach to solve it.`;

    const api = new llamacpp.Api({
        baseUrl: `http://localhost:${port}`,
    });

    let timeoutId = null;

    try {
        const timeout = 5000;
        const controller = new AbortController();
        timeoutId = setTimeout(() => {
            exports.log("ERROR", "‚ùå Timeout: Request took too long, aborting...");
            controller.abort();
        }, timeout);

        exports.log("INFO", "‚úÖ Requesting algorithm suggestion...");

        const textStream = await streamText({
            signal: controller.signal,
            model: new llamacpp.CompletionTextGenerator({
                api,
                temperature: 0,
                stopSequences: ["\n"],
            }),
            prompt: {
                system: llamaSystemPrompt,
                instruction: `Suggest an algorithm or approach to solve the following problem:\n${problemDescription}`,
                responsePrefix: "Suggested Algorithm:\n",
            },
        });

        let response = "";
        for await (const textPart of textStream) {
            if (!textPart) break;
            exports.log("INFO", `üìù Received: ${textPart}`);
            response += textPart;
        }

        clearTimeout(timeoutId);
        exports.log("INFO", "‚úÖ Response complete.");
        return response.trim() || "No response received.";

    } catch (error) {
        exports.log("ERROR", `‚ùå Error suggesting algorithm: ${error.message}`);
        if (timeoutId) clearTimeout(timeoutId);
        return "An error occurred while suggesting an algorithm.";
    }
};

// Express route handler to process API requests for algorithm suggestions
exports.generateAlgorithmSuggestion = async (req, res) => {
    const { problemDescription, port } = req.body;

    if (!problemDescription) {
        return res.status(400).json({ error: "Problem description is required." });
    }

    try {
        const response = await exports.suggestAlgorithm(problemDescription, port);
        return res.json({ algorithm: response });
    } catch (error) {
        exports.log("ERROR", `‚ùå Error generating algorithm suggestion: ${error.message}`);
        return res.status(500).json({ error: "An error occurred while generating the algorithm suggestion." });
    }
};

// Function to check if AI server is available
exports.checkServerAvailability = async (port) => {
    const serverUrl = `http://localhost:${port}`;
    try {
        await axios.get(serverUrl);
        exports.log("INFO", `‚úÖ AI server is ready at ${serverUrl}`);
        return true;
    } catch (error) {
        exports.log("ERROR", `‚ùå Server is unavailable at ${serverUrl}`);
        return false;
    }
};

/**
 * Generates a chatbot response using the AI model.
 * @param {string} prompt - The user's input.
 * @param {number} port - The AI server port (default: 4000).
 * @returns {Promise<string>} - AI-generated chatbot response.
 */
exports.getChatbotResponse = async function (prompt, port = 4000) {
    const timeout = 5000;
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), timeout);

    try {
        console.log("INFO: Generating chatbot response...");

        const isServerReady = await exports.checkServerAvailability(port);
        if (!isServerReady) {
            return "Server is unavailable. Please try again later.";
        }

        const serverUrl = `http://localhost:${port}`;
        const api = new llamacpp.Api({ serverUrl });

        const textStream = await streamText({
            signal: controller.signal,
            model: new llamacpp.CompletionTextGenerator({
                api,
                temperature: 0.7, // Slight randomness for more natural responses
                stopSequences: ["\n"], // Ensures cleaner responses
            }).withInstructionPrompt(),
            prompt: {
                system: "You are an AI chatbot that provides helpful and concise responses.",
                instruction: prompt,
                responsePrefix: "AI:",
            },
        });

        let response = "";
        for await (const textPart of textStream) {
            response += textPart;
        }

        clearTimeout(timeoutId);
        console.log("INFO: Chatbot response generated.");
        return response.trim() || "No response received.";
    } catch (error) {
        clearTimeout(timeoutId);
        console.error(`ERROR: Error generating chatbot response: ${error.message}`);
        return `An error occurred: ${error.message}`;
    }
};

/**
 * Express route handler for chatbot responses.
 */
exports.generateChatbotResponse = async function (req, res) {
    console.log("Incoming Request Body:", req.body);
    const { prompt, port } = req.body;

    if (!prompt) {
        return res.status(400).json({ error: "Prompt is required." });
    }

    try {
        const response = await exports.getChatbotResponse(prompt, port);
        return res.json({ response });
    } catch (error) {
        console.error(`ERROR: Error generating chatbot response: ${error.message}`);
        return res.status(500).json({ error: "An error occurred while processing your request." });
    }
};

// Unit Test Generation
// Function to refresh AI models (placeholder)
exports.refreshModel = async (req, res) => {
    try {
        exports.log("INFO", "üîÑ Refreshing AI models...");
        return res.status(200).json({ message: "Models refreshed successfully." });
    } catch (error) {
        exports.log("ERROR", `‚ùå Error refreshing models: ${error.message}`);
        return res.status(500).json({ error: "Error refreshing AI models." });
    }
};

// Function to check server health
exports.checkServerHealth = (req, res) => {
    try {
        return res.status(200).json({ status: "‚úÖ Server is up and running" });
    } catch (error) {
        exports.log("ERROR", `‚ùå Error checking server health: ${error.message}`);
        return res.status(500).json({ error: "Error checking server health." });
    }
};

// Exported helper function to generate unit tests for code
exports.generateUnitTestsForCode = async (code, language, port = 4000) => {
    console.log("Incoming Request Body:", req.body);
    const llamaSystemPrompt =
        `You are an AI assistant specializing in generating unit tests. ` +
        `Given the following code, generate unit tests that ensure the correctness of the code.`;

    // Ensure port is a valid number, defaulting to 4000 if invalid
    const validPort = Number.isNaN(Number(port)) ? 4000 : Number(port);

    const api = llamacpp.Api({
        baseUrl: {
            host: "localhost",
            port: validPort,
        },
    });

    try {
        const timeout = 5000; // 5 seconds timeout
        const controller = new AbortController();
        const timeoutId = setTimeout(() => controller.abort(), timeout);

        const textStream = await streamText({
            signal: controller.signal,
            model: llamacpp
                .CompletionTextGenerator({
                    api: api,
                    temperature: 0,
                    stopSequences: ["\n"],
                })
                .withInstructionPrompt(),
            prompt: {
                system: llamaSystemPrompt,
                instruction: `Please generate unit tests for the following ${language} code:\n\`\`\`${language}\n${code}\n\`\`\``,
                responsePrefix: "Unit Tests: ",
            },
        });

        let response = "";
        for await (const textPart of textStream) {
            response += textPart;
        }

        clearTimeout(timeoutId);
        return response;
    } catch (error) {
        exports.log("ERROR", `‚ùå Error generating unit tests: ${error.message}`);
        clearTimeout(timeoutId);
        return "An error occurred while generating unit tests.";
    }
};

// Exported controller function for handling chatbot generation request
// Function to generate unit tests for given code and language
exports.generateUnitTests = async (req, res) => {
    console.log("Incoming Request Body:", req.body);
    const { code, language, port } = req.body;

    if (!code || !language) {
        return res.status(400).json({ error: 'Code and language are required.' });
    }

    try {
        const unitTestResponse = await exports.generateUnitTestsForCode(code, language, port);
        return res.json({ unitTests: unitTestResponse });
    } catch (error) {
        exports.log("ERROR", `‚ùå Error generating unit tests: ${error.message}`);
        return res.status(500).json({ error: 'An error occurred while generating unit tests.' });
    }
};