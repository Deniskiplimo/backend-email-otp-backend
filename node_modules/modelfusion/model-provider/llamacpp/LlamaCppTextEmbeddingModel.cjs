"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.LlamaCppTextEmbeddingModel = void 0;
const zod_1 = require("zod");
const callWithRetryAndThrottle_js_1 = require("../../core/api/callWithRetryAndThrottle.cjs");
const postToApi_js_1 = require("../../core/api/postToApi.cjs");
const AbstractModel_js_1 = require("../../model-function/AbstractModel.cjs");
const LlamaCppApiConfiguration_js_1 = require("./LlamaCppApiConfiguration.cjs");
const LlamaCppError_js_1 = require("./LlamaCppError.cjs");
const LlamaCppTokenizer_js_1 = require("./LlamaCppTokenizer.cjs");
class LlamaCppTextEmbeddingModel extends AbstractModel_js_1.AbstractModel {
    constructor(settings = {}) {
        super({ settings });
        Object.defineProperty(this, "provider", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "llamacpp"
        });
        Object.defineProperty(this, "maxValuesPerCall", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: 1
        });
        Object.defineProperty(this, "contextWindowSize", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: undefined
        });
        Object.defineProperty(this, "embeddingDimensions", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "tokenizer", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        this.tokenizer = new LlamaCppTokenizer_js_1.LlamaCppTokenizer(this.settings.api);
        this.embeddingDimensions = this.settings.embeddingDimensions;
    }
    get modelName() {
        return null;
    }
    async tokenize(text) {
        return this.tokenizer.tokenize(text);
    }
    async callAPI(texts, options) {
        if (texts.length > this.maxValuesPerCall) {
            throw new Error(`The Llama.cpp embedding API only supports ${this.maxValuesPerCall} texts per API call.`);
        }
        return (0, callWithRetryAndThrottle_js_1.callWithRetryAndThrottle)({
            retry: this.settings.api?.retry,
            throttle: this.settings.api?.throttle,
            call: async () => callLlamaCppEmbeddingAPI({
                ...this.settings,
                abortSignal: options?.run?.abortSignal,
                content: texts[0],
            }),
        });
    }
    get settingsForEvent() {
        return {
            embeddingDimensions: this.settings.embeddingDimensions,
        };
    }
    async doEmbedValues(texts, options) {
        const response = await this.callAPI(texts, options);
        return {
            response,
            embeddings: [response.embedding],
        };
    }
    withSettings(additionalSettings) {
        return new LlamaCppTextEmbeddingModel(Object.assign({}, this.settings, additionalSettings));
    }
}
exports.LlamaCppTextEmbeddingModel = LlamaCppTextEmbeddingModel;
const llamaCppTextEmbeddingResponseSchema = zod_1.z.object({
    embedding: zod_1.z.array(zod_1.z.number()),
});
async function callLlamaCppEmbeddingAPI({ api = new LlamaCppApiConfiguration_js_1.LlamaCppApiConfiguration(), abortSignal, content, }) {
    return (0, postToApi_js_1.postJsonToApi)({
        url: api.assembleUrl(`/embedding`),
        headers: api.headers,
        body: { content },
        failedResponseHandler: LlamaCppError_js_1.failedLlamaCppCallResponseHandler,
        successfulResponseHandler: (0, postToApi_js_1.createJsonResponseHandler)(llamaCppTextEmbeddingResponseSchema),
        abortSignal,
    });
}
