"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.LlamaCppTextGenerationResponseFormat = exports.LlamaCppTextGenerationModel = void 0;
const zod_1 = require("zod");
const callWithRetryAndThrottle_js_1 = require("../../core/api/callWithRetryAndThrottle.cjs");
const postToApi_js_1 = require("../../core/api/postToApi.cjs");
const AsyncQueue_js_1 = require("../../event-source/AsyncQueue.cjs");
const parseEventSourceStream_js_1 = require("../../event-source/parseEventSourceStream.cjs");
const AbstractModel_js_1 = require("../../model-function/AbstractModel.cjs");
const PromptFormatTextStreamingModel_js_1 = require("../../model-function/generate-text/PromptFormatTextStreamingModel.cjs");
const parseJSON_js_1 = require("../../util/parseJSON.cjs");
const LlamaCppApiConfiguration_js_1 = require("./LlamaCppApiConfiguration.cjs");
const LlamaCppError_js_1 = require("./LlamaCppError.cjs");
const LlamaCppTokenizer_js_1 = require("./LlamaCppTokenizer.cjs");
class LlamaCppTextGenerationModel extends AbstractModel_js_1.AbstractModel {
    constructor(settings = {}) {
        super({ settings });
        Object.defineProperty(this, "provider", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "llamacpp"
        });
        Object.defineProperty(this, "tokenizer", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        this.tokenizer = new LlamaCppTokenizer_js_1.LlamaCppTokenizer(this.settings.api);
    }
    get modelName() {
        return null;
    }
    get contextWindowSize() {
        return this.settings.contextWindowSize;
    }
    async callAPI(prompt, options) {
        return (0, callWithRetryAndThrottle_js_1.callWithRetryAndThrottle)({
            retry: this.settings.api?.retry,
            throttle: this.settings.api?.throttle,
            call: async () => callLlamaCppTextGenerationAPI({
                ...this.settings,
                // mapping
                nPredict: this.settings.maxCompletionTokens,
                stop: this.settings.stopSequences,
                // other
                abortSignal: options.run?.abortSignal,
                prompt,
                responseFormat: options.responseFormat,
            }),
        });
    }
    get settingsForEvent() {
        const eventSettingProperties = [
            "maxCompletionTokens",
            "stopSequences",
            "contextWindowSize",
            "temperature",
            "topK",
            "topP",
            "nKeep",
            "tfsZ",
            "typicalP",
            "repeatPenalty",
            "repeatLastN",
            "penalizeNl",
            "mirostat",
            "mirostatTau",
            "mirostatEta",
            "seed",
            "ignoreEos",
            "logitBias",
        ];
        return Object.fromEntries(Object.entries(this.settings).filter(([key]) => eventSettingProperties.includes(key)));
    }
    async countPromptTokens(prompt) {
        const tokens = await this.tokenizer.tokenize(prompt);
        return tokens.length;
    }
    async doGenerateText(prompt, options) {
        const response = await this.callAPI(prompt, {
            ...options,
            responseFormat: exports.LlamaCppTextGenerationResponseFormat.json,
        });
        return {
            response,
            text: response.content,
            usage: {
                promptTokens: response.tokens_evaluated,
                completionTokens: response.tokens_predicted,
                totalTokens: response.tokens_evaluated + response.tokens_predicted,
            },
        };
    }
    doStreamText(prompt, options) {
        return this.callAPI(prompt, {
            ...options,
            responseFormat: exports.LlamaCppTextGenerationResponseFormat.deltaIterable,
        });
    }
    withPromptFormat(promptFormat) {
        return new PromptFormatTextStreamingModel_js_1.PromptFormatTextStreamingModel({
            model: this.withSettings({
                stopSequences: [
                    ...(this.settings.stopSequences ?? []),
                    ...promptFormat.stopSequences,
                ],
            }),
            promptFormat,
        });
    }
    withSettings(additionalSettings) {
        return new LlamaCppTextGenerationModel(Object.assign({}, this.settings, additionalSettings));
    }
}
exports.LlamaCppTextGenerationModel = LlamaCppTextGenerationModel;
const llamaCppTextGenerationResponseSchema = zod_1.z.object({
    content: zod_1.z.string(),
    stop: zod_1.z.literal(true),
    generation_settings: zod_1.z.object({
        frequency_penalty: zod_1.z.number(),
        ignore_eos: zod_1.z.boolean(),
        logit_bias: zod_1.z.array(zod_1.z.number()),
        mirostat: zod_1.z.number(),
        mirostat_eta: zod_1.z.number(),
        mirostat_tau: zod_1.z.number(),
        model: zod_1.z.string(),
        n_ctx: zod_1.z.number(),
        n_keep: zod_1.z.number(),
        n_predict: zod_1.z.number(),
        n_probs: zod_1.z.number(),
        penalize_nl: zod_1.z.boolean(),
        presence_penalty: zod_1.z.number(),
        repeat_last_n: zod_1.z.number(),
        repeat_penalty: zod_1.z.number(),
        seed: zod_1.z.number(),
        stop: zod_1.z.array(zod_1.z.string()),
        stream: zod_1.z.boolean(),
        temp: zod_1.z.number(),
        tfs_z: zod_1.z.number(),
        top_k: zod_1.z.number(),
        top_p: zod_1.z.number(),
        typical_p: zod_1.z.number(),
    }),
    model: zod_1.z.string(),
    prompt: zod_1.z.string(),
    stopped_eos: zod_1.z.boolean(),
    stopped_limit: zod_1.z.boolean(),
    stopped_word: zod_1.z.boolean(),
    stopping_word: zod_1.z.string(),
    timings: zod_1.z.object({
        predicted_ms: zod_1.z.number(),
        predicted_n: zod_1.z.number(),
        predicted_per_second: zod_1.z.number().nullable(),
        predicted_per_token_ms: zod_1.z.number().nullable(),
        prompt_ms: zod_1.z.number().nullable(),
        prompt_n: zod_1.z.number(),
        prompt_per_second: zod_1.z.number().nullable(),
        prompt_per_token_ms: zod_1.z.number().nullable(),
    }),
    tokens_cached: zod_1.z.number(),
    tokens_evaluated: zod_1.z.number(),
    tokens_predicted: zod_1.z.number(),
    truncated: zod_1.z.boolean(),
});
const llamaCppTextStreamingResponseSchema = zod_1.z.discriminatedUnion("stop", [
    zod_1.z.object({
        content: zod_1.z.string(),
        stop: zod_1.z.literal(false),
    }),
    llamaCppTextGenerationResponseSchema,
]);
async function callLlamaCppTextGenerationAPI({ api = new LlamaCppApiConfiguration_js_1.LlamaCppApiConfiguration(), abortSignal, responseFormat, prompt, temperature, topK, topP, nPredict, nKeep, stop, tfsZ, typicalP, repeatPenalty, repeatLastN, penalizeNl, mirostat, mirostatTau, mirostatEta, seed, ignoreEos, logitBias, }) {
    return (0, postToApi_js_1.postJsonToApi)({
        url: api.assembleUrl(`/completion`),
        headers: api.headers,
        body: {
            stream: responseFormat.stream,
            prompt,
            temperature,
            top_k: topK,
            top_p: topP,
            n_predict: nPredict,
            n_keep: nKeep,
            stop,
            tfs_z: tfsZ,
            typical_p: typicalP,
            repeat_penalty: repeatPenalty,
            repeat_last_n: repeatLastN,
            penalize_nl: penalizeNl,
            mirostat,
            mirostat_tau: mirostatTau,
            mirostat_eta: mirostatEta,
            seed,
            ignore_eos: ignoreEos,
            logit_bias: logitBias,
        },
        failedResponseHandler: LlamaCppError_js_1.failedLlamaCppCallResponseHandler,
        successfulResponseHandler: responseFormat.handler,
        abortSignal,
    });
}
async function createLlamaCppFullDeltaIterableQueue(stream) {
    const queue = new AsyncQueue_js_1.AsyncQueue();
    let content = "";
    // process the stream asynchonously (no 'await' on purpose):
    (0, parseEventSourceStream_js_1.parseEventSourceStream)({ stream })
        .then(async (events) => {
        try {
            for await (const event of events) {
                const data = event.data;
                const eventData = (0, parseJSON_js_1.parseJsonWithZod)(data, llamaCppTextStreamingResponseSchema);
                content += eventData.content;
                queue.push({
                    type: "delta",
                    fullDelta: {
                        content,
                        isComplete: eventData.stop,
                        delta: eventData.content,
                    },
                    valueDelta: eventData.content,
                });
                if (eventData.stop) {
                    queue.close();
                }
            }
        }
        catch (error) {
            queue.push({ type: "error", error });
            queue.close();
        }
    })
        .catch((error) => {
        queue.push({ type: "error", error });
        queue.close();
    });
    return queue;
}
exports.LlamaCppTextGenerationResponseFormat = {
    /**
     * Returns the response as a JSON object.
     */
    json: {
        stream: false,
        handler: (0, postToApi_js_1.createJsonResponseHandler)(llamaCppTextGenerationResponseSchema),
    },
    /**
     * Returns an async iterable over the full deltas (all choices, including full current state at time of event)
     * of the response stream.
     */
    deltaIterable: {
        stream: true,
        handler: async ({ response }) => createLlamaCppFullDeltaIterableQueue(response.body),
    },
};
