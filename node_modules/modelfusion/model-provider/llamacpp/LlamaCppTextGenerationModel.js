import { z } from "zod";
import { callWithRetryAndThrottle } from "../../core/api/callWithRetryAndThrottle.js";
import { createJsonResponseHandler, postJsonToApi, } from "../../core/api/postToApi.js";
import { AsyncQueue } from "../../event-source/AsyncQueue.js";
import { parseEventSourceStream } from "../../event-source/parseEventSourceStream.js";
import { AbstractModel } from "../../model-function/AbstractModel.js";
import { PromptFormatTextStreamingModel } from "../../model-function/generate-text/PromptFormatTextStreamingModel.js";
import { parseJsonWithZod } from "../../util/parseJSON.js";
import { LlamaCppApiConfiguration } from "./LlamaCppApiConfiguration.js";
import { failedLlamaCppCallResponseHandler } from "./LlamaCppError.js";
import { LlamaCppTokenizer } from "./LlamaCppTokenizer.js";
export class LlamaCppTextGenerationModel extends AbstractModel {
    constructor(settings = {}) {
        super({ settings });
        Object.defineProperty(this, "provider", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "llamacpp"
        });
        Object.defineProperty(this, "tokenizer", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        this.tokenizer = new LlamaCppTokenizer(this.settings.api);
    }
    get modelName() {
        return null;
    }
    get contextWindowSize() {
        return this.settings.contextWindowSize;
    }
    async callAPI(prompt, options) {
        return callWithRetryAndThrottle({
            retry: this.settings.api?.retry,
            throttle: this.settings.api?.throttle,
            call: async () => callLlamaCppTextGenerationAPI({
                ...this.settings,
                // mapping
                nPredict: this.settings.maxCompletionTokens,
                stop: this.settings.stopSequences,
                // other
                abortSignal: options.run?.abortSignal,
                prompt,
                responseFormat: options.responseFormat,
            }),
        });
    }
    get settingsForEvent() {
        const eventSettingProperties = [
            "maxCompletionTokens",
            "stopSequences",
            "contextWindowSize",
            "temperature",
            "topK",
            "topP",
            "nKeep",
            "tfsZ",
            "typicalP",
            "repeatPenalty",
            "repeatLastN",
            "penalizeNl",
            "mirostat",
            "mirostatTau",
            "mirostatEta",
            "seed",
            "ignoreEos",
            "logitBias",
        ];
        return Object.fromEntries(Object.entries(this.settings).filter(([key]) => eventSettingProperties.includes(key)));
    }
    async countPromptTokens(prompt) {
        const tokens = await this.tokenizer.tokenize(prompt);
        return tokens.length;
    }
    async doGenerateText(prompt, options) {
        const response = await this.callAPI(prompt, {
            ...options,
            responseFormat: LlamaCppTextGenerationResponseFormat.json,
        });
        return {
            response,
            text: response.content,
            usage: {
                promptTokens: response.tokens_evaluated,
                completionTokens: response.tokens_predicted,
                totalTokens: response.tokens_evaluated + response.tokens_predicted,
            },
        };
    }
    doStreamText(prompt, options) {
        return this.callAPI(prompt, {
            ...options,
            responseFormat: LlamaCppTextGenerationResponseFormat.deltaIterable,
        });
    }
    withPromptFormat(promptFormat) {
        return new PromptFormatTextStreamingModel({
            model: this.withSettings({
                stopSequences: [
                    ...(this.settings.stopSequences ?? []),
                    ...promptFormat.stopSequences,
                ],
            }),
            promptFormat,
        });
    }
    withSettings(additionalSettings) {
        return new LlamaCppTextGenerationModel(Object.assign({}, this.settings, additionalSettings));
    }
}
const llamaCppTextGenerationResponseSchema = z.object({
    content: z.string(),
    stop: z.literal(true),
    generation_settings: z.object({
        frequency_penalty: z.number(),
        ignore_eos: z.boolean(),
        logit_bias: z.array(z.number()),
        mirostat: z.number(),
        mirostat_eta: z.number(),
        mirostat_tau: z.number(),
        model: z.string(),
        n_ctx: z.number(),
        n_keep: z.number(),
        n_predict: z.number(),
        n_probs: z.number(),
        penalize_nl: z.boolean(),
        presence_penalty: z.number(),
        repeat_last_n: z.number(),
        repeat_penalty: z.number(),
        seed: z.number(),
        stop: z.array(z.string()),
        stream: z.boolean(),
        temp: z.number(),
        tfs_z: z.number(),
        top_k: z.number(),
        top_p: z.number(),
        typical_p: z.number(),
    }),
    model: z.string(),
    prompt: z.string(),
    stopped_eos: z.boolean(),
    stopped_limit: z.boolean(),
    stopped_word: z.boolean(),
    stopping_word: z.string(),
    timings: z.object({
        predicted_ms: z.number(),
        predicted_n: z.number(),
        predicted_per_second: z.number().nullable(),
        predicted_per_token_ms: z.number().nullable(),
        prompt_ms: z.number().nullable(),
        prompt_n: z.number(),
        prompt_per_second: z.number().nullable(),
        prompt_per_token_ms: z.number().nullable(),
    }),
    tokens_cached: z.number(),
    tokens_evaluated: z.number(),
    tokens_predicted: z.number(),
    truncated: z.boolean(),
});
const llamaCppTextStreamingResponseSchema = z.discriminatedUnion("stop", [
    z.object({
        content: z.string(),
        stop: z.literal(false),
    }),
    llamaCppTextGenerationResponseSchema,
]);
async function callLlamaCppTextGenerationAPI({ api = new LlamaCppApiConfiguration(), abortSignal, responseFormat, prompt, temperature, topK, topP, nPredict, nKeep, stop, tfsZ, typicalP, repeatPenalty, repeatLastN, penalizeNl, mirostat, mirostatTau, mirostatEta, seed, ignoreEos, logitBias, }) {
    return postJsonToApi({
        url: api.assembleUrl(`/completion`),
        headers: api.headers,
        body: {
            stream: responseFormat.stream,
            prompt,
            temperature,
            top_k: topK,
            top_p: topP,
            n_predict: nPredict,
            n_keep: nKeep,
            stop,
            tfs_z: tfsZ,
            typical_p: typicalP,
            repeat_penalty: repeatPenalty,
            repeat_last_n: repeatLastN,
            penalize_nl: penalizeNl,
            mirostat,
            mirostat_tau: mirostatTau,
            mirostat_eta: mirostatEta,
            seed,
            ignore_eos: ignoreEos,
            logit_bias: logitBias,
        },
        failedResponseHandler: failedLlamaCppCallResponseHandler,
        successfulResponseHandler: responseFormat.handler,
        abortSignal,
    });
}
async function createLlamaCppFullDeltaIterableQueue(stream) {
    const queue = new AsyncQueue();
    let content = "";
    // process the stream asynchonously (no 'await' on purpose):
    parseEventSourceStream({ stream })
        .then(async (events) => {
        try {
            for await (const event of events) {
                const data = event.data;
                const eventData = parseJsonWithZod(data, llamaCppTextStreamingResponseSchema);
                content += eventData.content;
                queue.push({
                    type: "delta",
                    fullDelta: {
                        content,
                        isComplete: eventData.stop,
                        delta: eventData.content,
                    },
                    valueDelta: eventData.content,
                });
                if (eventData.stop) {
                    queue.close();
                }
            }
        }
        catch (error) {
            queue.push({ type: "error", error });
            queue.close();
        }
    })
        .catch((error) => {
        queue.push({ type: "error", error });
        queue.close();
    });
    return queue;
}
export const LlamaCppTextGenerationResponseFormat = {
    /**
     * Returns the response as a JSON object.
     */
    json: {
        stream: false,
        handler: createJsonResponseHandler(llamaCppTextGenerationResponseSchema),
    },
    /**
     * Returns an async iterable over the full deltas (all choices, including full current state at time of event)
     * of the response stream.
     */
    deltaIterable: {
        stream: true,
        handler: async ({ response }) => createLlamaCppFullDeltaIterableQueue(response.body),
    },
};
