import { z } from "zod";
import { FunctionOptions } from "../../core/FunctionOptions.js";
import { ApiConfiguration } from "../../core/api/ApiConfiguration.js";
import { ResponseHandler } from "../../core/api/postToApi.js";
import { AbstractModel } from "../../model-function/AbstractModel.js";
import { Delta } from "../../model-function/Delta.js";
import { PromptFormatTextStreamingModel } from "../../model-function/generate-text/PromptFormatTextStreamingModel.js";
import { TextGenerationModelSettings, TextStreamingModel } from "../../model-function/generate-text/TextGenerationModel.js";
import { TextGenerationPromptFormat } from "../../model-function/generate-text/TextGenerationPromptFormat.js";
import { CohereTokenizer } from "./CohereTokenizer.js";
export declare const COHERE_TEXT_GENERATION_MODELS: {
    command: {
        contextWindowSize: number;
    };
    "command-nightly": {
        contextWindowSize: number;
    };
    "command-light": {
        contextWindowSize: number;
    };
    "command-light-nightly": {
        contextWindowSize: number;
    };
};
export type CohereTextGenerationModelType = keyof typeof COHERE_TEXT_GENERATION_MODELS;
export interface CohereTextGenerationModelSettings extends TextGenerationModelSettings {
    api?: ApiConfiguration;
    model: CohereTextGenerationModelType;
    numGenerations?: number;
    temperature?: number;
    k?: number;
    p?: number;
    frequencyPenalty?: number;
    presencePenalty?: number;
    returnLikelihoods?: "GENERATION" | "ALL" | "NONE";
    logitBias?: Record<string, number>;
    truncate?: "NONE" | "START" | "END";
    cohereStopSequences?: string[];
}
/**
 * Create a text generation model that calls the Cohere Co.Generate API.
 *
 * @see https://docs.cohere.com/reference/generate
 *
 * @example
 * const model = new CohereTextGenerationModel({
 *   model: "command-nightly",
 *   temperature: 0.7,
 *   maxCompletionTokens: 500,
 * });
 *
 * const text = await generateText(
 *    model,
 *   "Write a short story about a robot learning to love:\n\n"
 * );
 */
export declare class CohereTextGenerationModel extends AbstractModel<CohereTextGenerationModelSettings> implements TextStreamingModel<string, CohereTextGenerationModelSettings> {
    constructor(settings: CohereTextGenerationModelSettings);
    readonly provider: "cohere";
    get modelName(): "command" | "command-nightly" | "command-light" | "command-light-nightly";
    readonly contextWindowSize: number;
    readonly tokenizer: CohereTokenizer;
    countPromptTokens(input: string): Promise<number>;
    callAPI<RESPONSE>(prompt: string, options: {
        responseFormat: CohereTextGenerationResponseFormatType<RESPONSE>;
    } & FunctionOptions): Promise<RESPONSE>;
    get settingsForEvent(): Partial<CohereTextGenerationModelSettings>;
    doGenerateText(prompt: string, options?: FunctionOptions): Promise<{
        response: {
            prompt: string;
            id: string;
            generations: {
                text: string;
                id: string;
                finish_reason?: string | undefined;
            }[];
            meta?: {
                api_version: {
                    version: string;
                };
            } | undefined;
        };
        text: string;
    }>;
    doStreamText(prompt: string, options?: FunctionOptions): Promise<AsyncIterable<Delta<string>>>;
    extractTextDelta(fullDelta: CohereTextGenerationDelta): string | undefined;
    /**
     * Returns this model with an instruction prompt format.
     */
    withInstructionPrompt(): PromptFormatTextStreamingModel<import("../../index.js").InstructionPrompt, string, CohereTextGenerationModelSettings, this>;
    /**
     * Returns this model with a chat prompt format.
     */
    withChatPrompt(options?: {
        user?: string;
        ai?: string;
    }): PromptFormatTextStreamingModel<import("../../index.js").ChatPrompt, string, CohereTextGenerationModelSettings, this>;
    withPromptFormat<INPUT_PROMPT>(promptFormat: TextGenerationPromptFormat<INPUT_PROMPT, string>): PromptFormatTextStreamingModel<INPUT_PROMPT, string, CohereTextGenerationModelSettings, this>;
    withSettings(additionalSettings: Partial<CohereTextGenerationModelSettings>): this;
}
declare const cohereTextGenerationResponseSchema: z.ZodObject<{
    id: z.ZodString;
    generations: z.ZodArray<z.ZodObject<{
        id: z.ZodString;
        text: z.ZodString;
        finish_reason: z.ZodOptional<z.ZodString>;
    }, "strip", z.ZodTypeAny, {
        text: string;
        id: string;
        finish_reason?: string | undefined;
    }, {
        text: string;
        id: string;
        finish_reason?: string | undefined;
    }>, "many">;
    prompt: z.ZodString;
    meta: z.ZodOptional<z.ZodObject<{
        api_version: z.ZodObject<{
            version: z.ZodString;
        }, "strip", z.ZodTypeAny, {
            version: string;
        }, {
            version: string;
        }>;
    }, "strip", z.ZodTypeAny, {
        api_version: {
            version: string;
        };
    }, {
        api_version: {
            version: string;
        };
    }>>;
}, "strip", z.ZodTypeAny, {
    prompt: string;
    id: string;
    generations: {
        text: string;
        id: string;
        finish_reason?: string | undefined;
    }[];
    meta?: {
        api_version: {
            version: string;
        };
    } | undefined;
}, {
    prompt: string;
    id: string;
    generations: {
        text: string;
        id: string;
        finish_reason?: string | undefined;
    }[];
    meta?: {
        api_version: {
            version: string;
        };
    } | undefined;
}>;
export type CohereTextGenerationResponse = z.infer<typeof cohereTextGenerationResponseSchema>;
export type CohereTextGenerationDelta = {
    content: string;
    isComplete: boolean;
    delta: string;
};
export type CohereTextGenerationResponseFormatType<T> = {
    stream: boolean;
    handler: ResponseHandler<T>;
};
export declare const CohereTextGenerationResponseFormat: {
    /**
     * Returns the response as a JSON object.
     */
    json: {
        stream: false;
        handler: ResponseHandler<{
            prompt: string;
            id: string;
            generations: {
                text: string;
                id: string;
                finish_reason?: string | undefined;
            }[];
            meta?: {
                api_version: {
                    version: string;
                };
            } | undefined;
        }>;
    };
    /**
     * Returns an async iterable over the full deltas (all choices, including full current state at time of event)
     * of the response stream.
     */
    deltaIterable: {
        stream: true;
        handler: ({ response }: {
            response: Response;
        }) => Promise<AsyncIterable<Delta<string>>>;
    };
};
export {};
