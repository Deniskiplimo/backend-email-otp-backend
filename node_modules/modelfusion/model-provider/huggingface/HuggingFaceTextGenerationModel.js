import { z } from "zod";
import { callWithRetryAndThrottle } from "../../core/api/callWithRetryAndThrottle.js";
import { createJsonResponseHandler, postJsonToApi, } from "../../core/api/postToApi.js";
import { AbstractModel } from "../../model-function/AbstractModel.js";
import { PromptFormatTextGenerationModel } from "../../model-function/generate-text/PromptFormatTextGenerationModel.js";
import { HuggingFaceApiConfiguration } from "./HuggingFaceApiConfiguration.js";
import { failedHuggingFaceCallResponseHandler } from "./HuggingFaceError.js";
/**
 * Create a text generation model that calls a Hugging Face Inference API Text Generation Task.
 *
 * @see https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task
 *
 * @example
 * const model = new HuggingFaceTextGenerationModel({
 *   model: "tiiuae/falcon-7b",
 *   temperature: 0.7,
 *   maxCompletionTokens: 500,
 *   retry: retryWithExponentialBackoff({ maxTries: 5 }),
 * });
 *
 * const text = await generateText(
 *   model,
 *   "Write a short story about a robot learning to love:\n\n"
 * );
 */
export class HuggingFaceTextGenerationModel extends AbstractModel {
    constructor(settings) {
        super({ settings });
        Object.defineProperty(this, "provider", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "huggingface"
        });
        Object.defineProperty(this, "contextWindowSize", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: undefined
        });
        Object.defineProperty(this, "tokenizer", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: undefined
        });
        Object.defineProperty(this, "countPromptTokens", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: undefined
        });
    }
    get modelName() {
        return this.settings.model;
    }
    async callAPI(prompt, options) {
        return callWithRetryAndThrottle({
            retry: this.settings.api?.retry,
            throttle: this.settings.api?.throttle,
            call: async () => callHuggingFaceTextGenerationAPI({
                options: {
                    useCache: true,
                    waitForModel: true,
                },
                ...this.settings,
                maxNewTokens: this.settings.maxCompletionTokens,
                abortSignal: options?.run?.abortSignal,
                inputs: prompt,
            }),
        });
    }
    get settingsForEvent() {
        const eventSettingProperties = [
            "stopSequences",
            "maxCompletionTokens",
            "topK",
            "topP",
            "temperature",
            "repetitionPenalty",
            "maxTime",
            "numReturnSequences",
            "doSample",
            "options",
        ];
        return Object.fromEntries(Object.entries(this.settings).filter(([key]) => eventSettingProperties.includes(key)));
    }
    async doGenerateText(prompt, options) {
        const response = await this.callAPI(prompt, options);
        return {
            response,
            text: response[0].generated_text,
        };
    }
    withPromptFormat(promptFormat) {
        return new PromptFormatTextGenerationModel({
            model: this,
            promptFormat,
        });
    }
    withSettings(additionalSettings) {
        return new HuggingFaceTextGenerationModel(Object.assign({}, this.settings, additionalSettings));
    }
}
const huggingFaceTextGenerationResponseSchema = z.array(z.object({
    generated_text: z.string(),
}));
async function callHuggingFaceTextGenerationAPI({ api = new HuggingFaceApiConfiguration(), abortSignal, model, inputs, topK, topP, temperature, repetitionPenalty, maxNewTokens, maxTime, numReturnSequences, doSample, options, }) {
    return postJsonToApi({
        url: api.assembleUrl(`/${model}`),
        headers: api.headers,
        body: {
            inputs,
            top_k: topK,
            top_p: topP,
            temperature,
            repetition_penalty: repetitionPenalty,
            max_new_tokens: maxNewTokens,
            max_time: maxTime,
            num_return_sequences: numReturnSequences,
            do_sample: doSample,
            options: options
                ? {
                    use_cache: options?.useCache,
                    wait_for_model: options?.waitForModel,
                }
                : undefined,
        },
        failedResponseHandler: failedHuggingFaceCallResponseHandler,
        successfulResponseHandler: createJsonResponseHandler(huggingFaceTextGenerationResponseSchema),
        abortSignal,
    });
}
