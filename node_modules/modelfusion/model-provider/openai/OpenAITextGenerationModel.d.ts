import { z } from "zod";
import { FunctionOptions } from "../../core/FunctionOptions.js";
import { ApiConfiguration } from "../../core/api/ApiConfiguration.js";
import { ResponseHandler } from "../../core/api/postToApi.js";
import { AbstractModel } from "../../model-function/AbstractModel.js";
import { Delta } from "../../model-function/Delta.js";
import { PromptFormatTextStreamingModel } from "../../model-function/generate-text/PromptFormatTextStreamingModel.js";
import { TextGenerationModelSettings, TextStreamingModel } from "../../model-function/generate-text/TextGenerationModel.js";
import { TextGenerationPromptFormat } from "../../model-function/generate-text/TextGenerationPromptFormat.js";
import { TikTokenTokenizer } from "./TikTokenTokenizer.js";
/**
 * @see https://platform.openai.com/docs/models/
 * @see https://openai.com/pricing
 */
export declare const OPENAI_TEXT_GENERATION_MODELS: {
    "gpt-3.5-turbo-instruct": {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
    };
    "davinci-002": {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
        fineTunedTokenCostInMillicents: number;
    };
    "babbage-002": {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
        fineTunedTokenCostInMillicents: number;
    };
    "text-davinci-003": {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
    };
    "text-davinci-002": {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
    };
    "code-davinci-002": {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
    };
    davinci: {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
    };
    "text-curie-001": {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
    };
    curie: {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
    };
    "text-babbage-001": {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
    };
    babbage: {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
    };
    "text-ada-001": {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
    };
    ada: {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
    };
};
export declare function getOpenAITextGenerationModelInformation(model: OpenAITextGenerationModelType): {
    baseModel: OpenAITextGenerationBaseModelType;
    isFineTuned: boolean;
    contextWindowSize: number;
    promptTokenCostInMillicents: number;
    completionTokenCostInMillicents: number;
};
type FineTuneableOpenAITextGenerationModelType = "davinci-002" | "babbage-002";
type FineTunedOpenAITextGenerationModelType = `ft:${FineTuneableOpenAITextGenerationModelType}:${string}:${string}:${string}`;
export type OpenAITextGenerationBaseModelType = keyof typeof OPENAI_TEXT_GENERATION_MODELS;
export type OpenAITextGenerationModelType = OpenAITextGenerationBaseModelType | FineTunedOpenAITextGenerationModelType;
export declare const isOpenAITextGenerationModel: (model: string) => model is OpenAITextGenerationModelType;
export declare const calculateOpenAITextGenerationCostInMillicents: ({ model, response, }: {
    model: OpenAITextGenerationModelType;
    response: OpenAITextGenerationResponse;
}) => number;
export interface OpenAITextGenerationCallSettings {
    api?: ApiConfiguration;
    model: OpenAITextGenerationModelType;
    suffix?: string;
    maxTokens?: number;
    temperature?: number;
    topP?: number;
    n?: number;
    logprobs?: number;
    echo?: boolean;
    stop?: string | string[];
    presencePenalty?: number;
    frequencyPenalty?: number;
    bestOf?: number;
    logitBias?: Record<number, number>;
}
export interface OpenAITextGenerationModelSettings extends TextGenerationModelSettings, Omit<OpenAITextGenerationCallSettings, "stop" | "maxTokens"> {
    isUserIdForwardingEnabled?: boolean;
}
/**
 * Create a text generation model that calls the OpenAI text completion API.
 *
 * @see https://platform.openai.com/docs/api-reference/completions/create
 *
 * @example
 * const model = new OpenAITextGenerationModel({
 *   model: "gpt-3.5-turbo-instruct",
 *   temperature: 0.7,
 *   maxCompletionTokens: 500,
 *   retry: retryWithExponentialBackoff({ maxTries: 5 }),
 * });
 *
 * const text = await generateText(
 *   model,
 *   "Write a short story about a robot learning to love:\n\n"
 * );
 */
export declare class OpenAITextGenerationModel extends AbstractModel<OpenAITextGenerationModelSettings> implements TextStreamingModel<string, OpenAITextGenerationModelSettings> {
    constructor(settings: OpenAITextGenerationModelSettings);
    readonly provider: "openai";
    get modelName(): OpenAITextGenerationModelType;
    readonly contextWindowSize: number;
    readonly tokenizer: TikTokenTokenizer;
    countPromptTokens(input: string): Promise<number>;
    callAPI<RESULT>(prompt: string, options: {
        responseFormat: OpenAITextResponseFormatType<RESULT>;
    } & FunctionOptions): Promise<RESULT>;
    get settingsForEvent(): Partial<OpenAITextGenerationModelSettings>;
    doGenerateText(prompt: string, options?: FunctionOptions): Promise<{
        response: {
            object: "text_completion";
            model: string;
            usage: {
                prompt_tokens: number;
                completion_tokens: number;
                total_tokens: number;
            };
            id: string;
            created: number;
            choices: {
                text: string;
                finish_reason: string;
                index: number;
                logprobs?: any;
            }[];
        };
        text: string;
        usage: {
            promptTokens: number;
            completionTokens: number;
            totalTokens: number;
        };
    }>;
    doStreamText(prompt: string, options?: FunctionOptions): Promise<AsyncIterable<Delta<string>>>;
    /**
     * Returns this model with an instruction prompt format.
     */
    withInstructionPrompt(): PromptFormatTextStreamingModel<import("../../index.js").InstructionPrompt, string, OpenAITextGenerationModelSettings, this>;
    /**
     * Returns this model with a chat prompt format.
     */
    withChatPrompt(options?: {
        user?: string;
        ai?: string;
    }): PromptFormatTextStreamingModel<import("../../index.js").ChatPrompt, string, OpenAITextGenerationModelSettings, this>;
    withPromptFormat<INPUT_PROMPT>(promptFormat: TextGenerationPromptFormat<INPUT_PROMPT, string>): PromptFormatTextStreamingModel<INPUT_PROMPT, string, OpenAITextGenerationModelSettings, this>;
    withSettings(additionalSettings: Partial<OpenAITextGenerationModelSettings>): this;
}
declare const openAITextGenerationResponseSchema: z.ZodObject<{
    id: z.ZodString;
    object: z.ZodLiteral<"text_completion">;
    created: z.ZodNumber;
    model: z.ZodString;
    choices: z.ZodArray<z.ZodObject<{
        text: z.ZodString;
        index: z.ZodNumber;
        logprobs: z.ZodNullable<z.ZodAny>;
        finish_reason: z.ZodString;
    }, "strip", z.ZodTypeAny, {
        text: string;
        finish_reason: string;
        index: number;
        logprobs?: any;
    }, {
        text: string;
        finish_reason: string;
        index: number;
        logprobs?: any;
    }>, "many">;
    usage: z.ZodObject<{
        prompt_tokens: z.ZodNumber;
        completion_tokens: z.ZodNumber;
        total_tokens: z.ZodNumber;
    }, "strip", z.ZodTypeAny, {
        prompt_tokens: number;
        completion_tokens: number;
        total_tokens: number;
    }, {
        prompt_tokens: number;
        completion_tokens: number;
        total_tokens: number;
    }>;
}, "strip", z.ZodTypeAny, {
    object: "text_completion";
    model: string;
    usage: {
        prompt_tokens: number;
        completion_tokens: number;
        total_tokens: number;
    };
    id: string;
    created: number;
    choices: {
        text: string;
        finish_reason: string;
        index: number;
        logprobs?: any;
    }[];
}, {
    object: "text_completion";
    model: string;
    usage: {
        prompt_tokens: number;
        completion_tokens: number;
        total_tokens: number;
    };
    id: string;
    created: number;
    choices: {
        text: string;
        finish_reason: string;
        index: number;
        logprobs?: any;
    }[];
}>;
export type OpenAITextGenerationResponse = z.infer<typeof openAITextGenerationResponseSchema>;
export type OpenAITextResponseFormatType<T> = {
    stream: boolean;
    handler: ResponseHandler<T>;
};
export declare const OpenAITextResponseFormat: {
    /**
     * Returns the response as a JSON object.
     */
    json: {
        stream: false;
        handler: ResponseHandler<{
            object: "text_completion";
            model: string;
            usage: {
                prompt_tokens: number;
                completion_tokens: number;
                total_tokens: number;
            };
            id: string;
            created: number;
            choices: {
                text: string;
                finish_reason: string;
                index: number;
                logprobs?: any;
            }[];
        }>;
    };
    /**
     * Returns an async iterable over the full deltas (all choices, including full current state at time of event)
     * of the response stream.
     */
    deltaIterable: {
        stream: true;
        handler: ({ response }: {
            response: Response;
        }) => Promise<AsyncIterable<Delta<string>>>;
    };
};
export type OpenAITextGenerationDelta = Array<{
    content: string;
    isComplete: boolean;
    delta: string;
}>;
export {};
