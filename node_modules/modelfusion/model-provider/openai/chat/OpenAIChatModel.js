import SecureJSON from "secure-json-parse";
import { z } from "zod";
import { callWithRetryAndThrottle } from "../../../core/api/callWithRetryAndThrottle.js";
import { createJsonResponseHandler, postJsonToApi, } from "../../../core/api/postToApi.js";
import { AbstractModel } from "../../../model-function/AbstractModel.js";
import { StructureParseError } from "../../../model-function/generate-structure/StructureParseError.js";
import { parsePartialJson } from "../../../model-function/generate-structure/parsePartialJson.js";
import { PromptFormatTextStreamingModel } from "../../../model-function/generate-text/PromptFormatTextStreamingModel.js";
import { OpenAIApiConfiguration } from "../OpenAIApiConfiguration.js";
import { failedOpenAICallResponseHandler } from "../OpenAIError.js";
import { TikTokenTokenizer } from "../TikTokenTokenizer.js";
import { mapChatPromptToOpenAIChatFormat, mapInstructionPromptToOpenAIChatFormat, } from "./OpenAIChatPromptFormat.js";
import { createOpenAIChatDeltaIterableQueue } from "./OpenAIChatStreamIterable.js";
import { countOpenAIChatPromptTokens } from "./countOpenAIChatMessageTokens.js";
/*
 * Available OpenAI chat models, their token limits, and pricing.
 *
 * @see https://platform.openai.com/docs/models/
 * @see https://openai.com/pricing
 */
export const OPENAI_CHAT_MODELS = {
    "gpt-4": {
        contextWindowSize: 8192,
        promptTokenCostInMillicents: 3,
        completionTokenCostInMillicents: 6,
    },
    "gpt-4-0314": {
        contextWindowSize: 8192,
        promptTokenCostInMillicents: 3,
        completionTokenCostInMillicents: 6,
    },
    "gpt-4-0613": {
        contextWindowSize: 8192,
        promptTokenCostInMillicents: 3,
        completionTokenCostInMillicents: 6,
    },
    "gpt-4-32k": {
        contextWindowSize: 32768,
        promptTokenCostInMillicents: 6,
        completionTokenCostInMillicents: 12,
    },
    "gpt-4-32k-0314": {
        contextWindowSize: 32768,
        promptTokenCostInMillicents: 6,
        completionTokenCostInMillicents: 12,
    },
    "gpt-4-32k-0613": {
        contextWindowSize: 32768,
        promptTokenCostInMillicents: 6,
        completionTokenCostInMillicents: 12,
    },
    "gpt-3.5-turbo": {
        contextWindowSize: 4096,
        promptTokenCostInMillicents: 0.15,
        completionTokenCostInMillicents: 0.2,
        fineTunedPromptTokenCostInMillicents: 1.2,
        fineTunedCompletionTokenCostInMillicents: 1.6,
    },
    "gpt-3.5-turbo-0301": {
        contextWindowSize: 4096,
        promptTokenCostInMillicents: 0.15,
        completionTokenCostInMillicents: 0.2,
    },
    "gpt-3.5-turbo-0613": {
        contextWindowSize: 4096,
        promptTokenCostInMillicents: 0.15,
        completionTokenCostInMillicents: 0.2,
        fineTunedPromptTokenCostInMillicents: 1.2,
        fineTunedCompletionTokenCostInMillicents: 1.6,
    },
    "gpt-3.5-turbo-16k": {
        contextWindowSize: 16384,
        promptTokenCostInMillicents: 0.3,
        completionTokenCostInMillicents: 0.4,
    },
    "gpt-3.5-turbo-16k-0613": {
        contextWindowSize: 16384,
        promptTokenCostInMillicents: 0.3,
        completionTokenCostInMillicents: 0.4,
    },
};
export function getOpenAIChatModelInformation(model) {
    // Model is already a base model:
    if (model in OPENAI_CHAT_MODELS) {
        const baseModelInformation = OPENAI_CHAT_MODELS[model];
        return {
            baseModel: model,
            isFineTuned: false,
            contextWindowSize: baseModelInformation.contextWindowSize,
            promptTokenCostInMillicents: baseModelInformation.promptTokenCostInMillicents,
            completionTokenCostInMillicents: baseModelInformation.completionTokenCostInMillicents,
        };
    }
    // Extract the base model from the fine-tuned model:
    // eslint-disable-next-line @typescript-eslint/no-unused-vars
    const [_, baseModel, ___, ____, _____] = model.split(":");
    if (["gpt-3.5-turbo", "gpt-3.5-turbo-0613"].includes(baseModel)) {
        const baseModelInformation = OPENAI_CHAT_MODELS[baseModel];
        return {
            baseModel: baseModel,
            isFineTuned: true,
            contextWindowSize: baseModelInformation.contextWindowSize,
            promptTokenCostInMillicents: baseModelInformation.fineTunedPromptTokenCostInMillicents,
            completionTokenCostInMillicents: baseModelInformation.fineTunedCompletionTokenCostInMillicents,
        };
    }
    throw new Error(`Unknown OpenAI chat base model ${baseModel}.`);
}
export const isOpenAIChatModel = (model) => model in OPENAI_CHAT_MODELS ||
    model.startsWith("ft:gpt-3.5-turbo-0613:") ||
    model.startsWith("ft:gpt-3.5-turbo:");
export const calculateOpenAIChatCostInMillicents = ({ model, response, }) => {
    const modelInformation = getOpenAIChatModelInformation(model);
    return (response.usage.prompt_tokens *
        modelInformation.promptTokenCostInMillicents +
        response.usage.completion_tokens *
            modelInformation.completionTokenCostInMillicents);
};
/**
 * Create a text generation model that calls the OpenAI chat completion API.
 *
 * @see https://platform.openai.com/docs/api-reference/chat/create
 *
 * @example
 * const model = new OpenAIChatModel({
 *   model: "gpt-3.5-turbo",
 *   temperature: 0.7,
 *   maxCompletionTokens: 500,
 * });
 *
 * const text = await generateText([
 *   model,
 *   OpenAIChatMessage.system(
 *     "Write a short story about a robot learning to love:"
 *   ),
 * ]);
 */
export class OpenAIChatModel extends AbstractModel {
    constructor(settings) {
        super({ settings });
        Object.defineProperty(this, "provider", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "openai"
        });
        Object.defineProperty(this, "contextWindowSize", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "tokenizer", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        const modelInformation = getOpenAIChatModelInformation(this.settings.model);
        this.tokenizer = new TikTokenTokenizer({
            model: modelInformation.baseModel,
        });
        this.contextWindowSize = modelInformation.contextWindowSize;
    }
    get modelName() {
        return this.settings.model;
    }
    /**
     * Counts the prompt tokens required for the messages. This includes the message base tokens
     * and the prompt base tokens.
     */
    countPromptTokens(messages) {
        return countOpenAIChatPromptTokens({
            messages,
            model: this.modelName,
        });
    }
    async callAPI(messages, options) {
        return callWithRetryAndThrottle({
            retry: this.settings.api?.retry,
            throttle: this.settings.api?.throttle,
            call: async () => callOpenAIChatCompletionAPI({
                ...this.settings,
                // function calling:
                functions: options.functions ?? this.settings.functions,
                functionCall: options.functionCall ?? this.settings.functionCall,
                // map to OpenAI API names:
                stop: this.settings.stopSequences,
                maxTokens: this.settings.maxCompletionTokens,
                // other settings:
                user: this.settings.isUserIdForwardingEnabled
                    ? options.run?.userId
                    : undefined,
                abortSignal: options.run?.abortSignal,
                responseFormat: options.responseFormat,
                messages,
            }),
        });
    }
    get settingsForEvent() {
        const eventSettingProperties = [
            "stopSequences",
            "maxCompletionTokens",
            "functions",
            "functionCall",
            "temperature",
            "topP",
            "n",
            "presencePenalty",
            "frequencyPenalty",
            "logitBias",
        ];
        return Object.fromEntries(Object.entries(this.settings).filter(([key]) => eventSettingProperties.includes(key)));
    }
    async doGenerateText(prompt, options) {
        const response = await this.callAPI(prompt, {
            ...options,
            responseFormat: OpenAIChatResponseFormat.json,
        });
        return {
            response,
            text: response.choices[0].message.content,
            usage: this.extractUsage(response),
        };
    }
    doStreamText(prompt, options) {
        return this.callAPI(prompt, {
            ...options,
            responseFormat: OpenAIChatResponseFormat.textDeltaIterable,
        });
    }
    /**
     * JSON generation uses the OpenAI GPT function calling API.
     * It provides a single function specification and instructs the model to provide parameters for calling the function.
     * The result is returned as parsed JSON.
     *
     * @see https://platform.openai.com/docs/guides/gpt/function-calling
     */
    async doGenerateStructure(structureDefinition, prompt, options) {
        const response = await this.callAPI(prompt, {
            ...options,
            responseFormat: OpenAIChatResponseFormat.json,
            functionCall: { name: structureDefinition.name },
            functions: [
                {
                    name: structureDefinition.name,
                    description: structureDefinition.description,
                    parameters: structureDefinition.schema.getJsonSchema(),
                },
            ],
        });
        const valueText = response.choices[0].message.function_call.arguments;
        try {
            return {
                response,
                valueText,
                value: SecureJSON.parse(valueText),
                usage: this.extractUsage(response),
            };
        }
        catch (error) {
            throw new StructureParseError({
                structureName: structureDefinition.name,
                valueText,
                cause: error,
            });
        }
    }
    async doStreamStructure(structureDefinition, prompt, options) {
        return this.callAPI(prompt, {
            ...options,
            responseFormat: OpenAIChatResponseFormat.structureDeltaIterable,
            functionCall: { name: structureDefinition.name },
            functions: [
                {
                    name: structureDefinition.name,
                    description: structureDefinition.description,
                    parameters: structureDefinition.schema.getJsonSchema(),
                },
            ],
        });
    }
    async doGenerateStructureOrText(structureDefinitions, prompt, options) {
        const response = await this.callAPI(prompt, {
            ...options,
            responseFormat: OpenAIChatResponseFormat.json,
            functionCall: "auto",
            functions: structureDefinitions.map((structureDefinition) => ({
                name: structureDefinition.name,
                description: structureDefinition.description,
                parameters: structureDefinition.schema.getJsonSchema(),
            })),
        });
        const message = response.choices[0].message;
        const content = message.content;
        const functionCall = message.function_call;
        if (functionCall == null) {
            return {
                response,
                structureAndText: {
                    structure: null,
                    value: null,
                    valueText: null,
                    text: content ?? "",
                },
                usage: this.extractUsage(response),
            };
        }
        try {
            return {
                response,
                structureAndText: {
                    structure: functionCall.name,
                    value: SecureJSON.parse(functionCall.arguments),
                    valueText: functionCall.arguments,
                    text: content,
                },
                usage: this.extractUsage(response),
            };
        }
        catch (error) {
            throw new StructureParseError({
                structureName: functionCall.name,
                valueText: functionCall.arguments,
                cause: error,
            });
        }
    }
    extractUsage(response) {
        return {
            promptTokens: response.usage.prompt_tokens,
            completionTokens: response.usage.completion_tokens,
            totalTokens: response.usage.total_tokens,
        };
    }
    /**
     * Returns this model with an instruction prompt format.
     */
    withInstructionPrompt() {
        return this.withPromptFormat(mapInstructionPromptToOpenAIChatFormat());
    }
    /**
     * Returns this model with a chat prompt format.
     */
    withChatPrompt() {
        return this.withPromptFormat(mapChatPromptToOpenAIChatFormat());
    }
    withPromptFormat(promptFormat) {
        return new PromptFormatTextStreamingModel({
            model: this.withSettings({
                stopSequences: [
                    ...(this.settings.stopSequences ?? []),
                    ...promptFormat.stopSequences,
                ],
            }),
            promptFormat,
        });
    }
    withSettings(additionalSettings) {
        return new OpenAIChatModel(Object.assign({}, this.settings, additionalSettings));
    }
}
const openAIChatResponseSchema = z.object({
    id: z.string(),
    object: z.literal("chat.completion"),
    created: z.number(),
    model: z.string(),
    choices: z.array(z.object({
        message: z.object({
            role: z.literal("assistant"),
            content: z.string().nullable(),
            function_call: z
                .object({
                name: z.string(),
                arguments: z.string(),
            })
                .optional(),
        }),
        index: z.number(),
        logprobs: z.nullable(z.any()),
        finish_reason: z.string(),
    })),
    usage: z.object({
        prompt_tokens: z.number(),
        completion_tokens: z.number(),
        total_tokens: z.number(),
    }),
});
async function callOpenAIChatCompletionAPI({ api = new OpenAIApiConfiguration(), abortSignal, responseFormat, model, messages, functions, functionCall, temperature, topP, n, stop, maxTokens, presencePenalty, frequencyPenalty, logitBias, user, }) {
    // empty arrays are not allowed for stop:
    if (stop != null && Array.isArray(stop) && stop.length === 0) {
        stop = undefined;
    }
    return postJsonToApi({
        url: api.assembleUrl("/chat/completions"),
        headers: api.headers,
        body: {
            stream: responseFormat.stream,
            model,
            messages,
            functions,
            function_call: functionCall,
            temperature,
            top_p: topP,
            n,
            stop,
            max_tokens: maxTokens,
            presence_penalty: presencePenalty,
            frequency_penalty: frequencyPenalty,
            logit_bias: logitBias,
            user,
        },
        failedResponseHandler: failedOpenAICallResponseHandler,
        successfulResponseHandler: responseFormat.handler,
        abortSignal,
    });
}
export const OpenAIChatResponseFormat = {
    /**
     * Returns the response as a JSON object.
     */
    json: {
        stream: false,
        handler: createJsonResponseHandler(openAIChatResponseSchema),
    },
    /**
     * Returns an async iterable over the text deltas (only the tex different of the first choice).
     */
    textDeltaIterable: {
        stream: true,
        handler: async ({ response }) => createOpenAIChatDeltaIterableQueue(response.body, (delta) => delta[0]?.delta.content ?? ""),
    },
    structureDeltaIterable: {
        stream: true,
        handler: async ({ response }) => createOpenAIChatDeltaIterableQueue(response.body, (delta) => parsePartialJson(delta[0]?.function_call?.arguments)),
    },
};
