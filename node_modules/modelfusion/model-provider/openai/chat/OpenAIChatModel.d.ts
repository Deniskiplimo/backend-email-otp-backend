import { z } from "zod";
import { FunctionOptions } from "../../../core/FunctionOptions.js";
import { ApiConfiguration } from "../../../core/api/ApiConfiguration.js";
import { ResponseHandler } from "../../../core/api/postToApi.js";
import { StructureDefinition } from "../../../core/structure/StructureDefinition.js";
import { AbstractModel } from "../../../model-function/AbstractModel.js";
import { Delta } from "../../../model-function/Delta.js";
import { StructureGenerationModel } from "../../../model-function/generate-structure/StructureGenerationModel.js";
import { StructureOrTextGenerationModel } from "../../../model-function/generate-structure/StructureOrTextGenerationModel.js";
import { PromptFormatTextStreamingModel } from "../../../model-function/generate-text/PromptFormatTextStreamingModel.js";
import { TextGenerationModelSettings, TextStreamingModel } from "../../../model-function/generate-text/TextGenerationModel.js";
import { TextGenerationPromptFormat } from "../../../model-function/generate-text/TextGenerationPromptFormat.js";
import { TikTokenTokenizer } from "../TikTokenTokenizer.js";
import { OpenAIChatMessage } from "./OpenAIChatMessage.js";
export declare const OPENAI_CHAT_MODELS: {
    "gpt-4": {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
    };
    "gpt-4-0314": {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
    };
    "gpt-4-0613": {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
    };
    "gpt-4-32k": {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
    };
    "gpt-4-32k-0314": {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
    };
    "gpt-4-32k-0613": {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
    };
    "gpt-3.5-turbo": {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
        fineTunedPromptTokenCostInMillicents: number;
        fineTunedCompletionTokenCostInMillicents: number;
    };
    "gpt-3.5-turbo-0301": {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
    };
    "gpt-3.5-turbo-0613": {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
        fineTunedPromptTokenCostInMillicents: number;
        fineTunedCompletionTokenCostInMillicents: number;
    };
    "gpt-3.5-turbo-16k": {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
    };
    "gpt-3.5-turbo-16k-0613": {
        contextWindowSize: number;
        promptTokenCostInMillicents: number;
        completionTokenCostInMillicents: number;
    };
};
export declare function getOpenAIChatModelInformation(model: OpenAIChatModelType): {
    baseModel: OpenAIChatBaseModelType;
    isFineTuned: boolean;
    contextWindowSize: number;
    promptTokenCostInMillicents: number;
    completionTokenCostInMillicents: number;
};
type FineTuneableOpenAIChatModelType = `gpt-3.5-turbo` | `gpt-3.5-turbo-0613`;
type FineTunedOpenAIChatModelType = `ft:${FineTuneableOpenAIChatModelType}:${string}:${string}:${string}`;
export type OpenAIChatBaseModelType = keyof typeof OPENAI_CHAT_MODELS;
export type OpenAIChatModelType = OpenAIChatBaseModelType | FineTunedOpenAIChatModelType;
export declare const isOpenAIChatModel: (model: string) => model is OpenAIChatModelType;
export declare const calculateOpenAIChatCostInMillicents: ({ model, response, }: {
    model: OpenAIChatModelType;
    response: OpenAIChatResponse;
}) => number;
export interface OpenAIChatCallSettings {
    api?: ApiConfiguration;
    model: OpenAIChatModelType;
    functions?: Array<{
        name: string;
        description?: string;
        parameters: unknown;
    }>;
    functionCall?: "none" | "auto" | {
        name: string;
    };
    stop?: string | string[];
    maxTokens?: number;
    temperature?: number;
    topP?: number;
    n?: number;
    presencePenalty?: number;
    frequencyPenalty?: number;
    logitBias?: Record<number, number>;
}
export interface OpenAIChatSettings extends TextGenerationModelSettings, Omit<OpenAIChatCallSettings, "stop" | "maxTokens"> {
    isUserIdForwardingEnabled?: boolean;
}
/**
 * Create a text generation model that calls the OpenAI chat completion API.
 *
 * @see https://platform.openai.com/docs/api-reference/chat/create
 *
 * @example
 * const model = new OpenAIChatModel({
 *   model: "gpt-3.5-turbo",
 *   temperature: 0.7,
 *   maxCompletionTokens: 500,
 * });
 *
 * const text = await generateText([
 *   model,
 *   OpenAIChatMessage.system(
 *     "Write a short story about a robot learning to love:"
 *   ),
 * ]);
 */
export declare class OpenAIChatModel extends AbstractModel<OpenAIChatSettings> implements TextStreamingModel<OpenAIChatMessage[], OpenAIChatSettings>, StructureGenerationModel<OpenAIChatMessage[], OpenAIChatSettings>, StructureOrTextGenerationModel<OpenAIChatMessage[], OpenAIChatSettings> {
    constructor(settings: OpenAIChatSettings);
    readonly provider: "openai";
    get modelName(): OpenAIChatModelType;
    readonly contextWindowSize: number;
    readonly tokenizer: TikTokenTokenizer;
    /**
     * Counts the prompt tokens required for the messages. This includes the message base tokens
     * and the prompt base tokens.
     */
    countPromptTokens(messages: OpenAIChatMessage[]): Promise<number>;
    callAPI<RESULT>(messages: Array<OpenAIChatMessage>, options: {
        responseFormat: OpenAIChatResponseFormatType<RESULT>;
    } & FunctionOptions & {
        functions?: Array<{
            name: string;
            description?: string;
            parameters: unknown;
        }>;
        functionCall?: "none" | "auto" | {
            name: string;
        };
    }): Promise<RESULT>;
    get settingsForEvent(): Partial<OpenAIChatSettings>;
    doGenerateText(prompt: OpenAIChatMessage[], options?: FunctionOptions): Promise<{
        response: {
            object: "chat.completion";
            model: string;
            usage: {
                prompt_tokens: number;
                completion_tokens: number;
                total_tokens: number;
            };
            id: string;
            created: number;
            choices: {
                message: {
                    content: string | null;
                    role: "assistant";
                    function_call?: {
                        name: string;
                        arguments: string;
                    } | undefined;
                };
                finish_reason: string;
                index: number;
                logprobs?: any;
            }[];
        };
        text: string;
        usage: {
            promptTokens: number;
            completionTokens: number;
            totalTokens: number;
        };
    }>;
    doStreamText(prompt: OpenAIChatMessage[], options?: FunctionOptions): Promise<AsyncIterable<Delta<string>>>;
    /**
     * JSON generation uses the OpenAI GPT function calling API.
     * It provides a single function specification and instructs the model to provide parameters for calling the function.
     * The result is returned as parsed JSON.
     *
     * @see https://platform.openai.com/docs/guides/gpt/function-calling
     */
    doGenerateStructure(structureDefinition: StructureDefinition<string, unknown>, prompt: OpenAIChatMessage[], options?: FunctionOptions): Promise<{
        response: {
            object: "chat.completion";
            model: string;
            usage: {
                prompt_tokens: number;
                completion_tokens: number;
                total_tokens: number;
            };
            id: string;
            created: number;
            choices: {
                message: {
                    content: string | null;
                    role: "assistant";
                    function_call?: {
                        name: string;
                        arguments: string;
                    } | undefined;
                };
                finish_reason: string;
                index: number;
                logprobs?: any;
            }[];
        };
        valueText: string;
        value: any;
        usage: {
            promptTokens: number;
            completionTokens: number;
            totalTokens: number;
        };
    }>;
    doStreamStructure(structureDefinition: StructureDefinition<string, unknown>, prompt: OpenAIChatMessage[], options?: FunctionOptions): Promise<AsyncIterable<Delta<unknown>>>;
    doGenerateStructureOrText(structureDefinitions: Array<StructureDefinition<string, unknown>>, prompt: OpenAIChatMessage[], options?: FunctionOptions): Promise<{
        response: {
            object: "chat.completion";
            model: string;
            usage: {
                prompt_tokens: number;
                completion_tokens: number;
                total_tokens: number;
            };
            id: string;
            created: number;
            choices: {
                message: {
                    content: string | null;
                    role: "assistant";
                    function_call?: {
                        name: string;
                        arguments: string;
                    } | undefined;
                };
                finish_reason: string;
                index: number;
                logprobs?: any;
            }[];
        };
        structureAndText: {
            structure: null;
            value: null;
            valueText: null;
            text: string;
        };
        usage: {
            promptTokens: number;
            completionTokens: number;
            totalTokens: number;
        };
    } | {
        response: {
            object: "chat.completion";
            model: string;
            usage: {
                prompt_tokens: number;
                completion_tokens: number;
                total_tokens: number;
            };
            id: string;
            created: number;
            choices: {
                message: {
                    content: string | null;
                    role: "assistant";
                    function_call?: {
                        name: string;
                        arguments: string;
                    } | undefined;
                };
                finish_reason: string;
                index: number;
                logprobs?: any;
            }[];
        };
        structureAndText: {
            structure: string;
            value: any;
            valueText: string;
            text: string | null;
        };
        usage: {
            promptTokens: number;
            completionTokens: number;
            totalTokens: number;
        };
    }>;
    extractUsage(response: OpenAIChatResponse): {
        promptTokens: number;
        completionTokens: number;
        totalTokens: number;
    };
    /**
     * Returns this model with an instruction prompt format.
     */
    withInstructionPrompt(): PromptFormatTextStreamingModel<import("../../../index.js").InstructionPrompt, OpenAIChatMessage[], OpenAIChatSettings, this>;
    /**
     * Returns this model with a chat prompt format.
     */
    withChatPrompt(): PromptFormatTextStreamingModel<import("../../../index.js").ChatPrompt, OpenAIChatMessage[], OpenAIChatSettings, this>;
    withPromptFormat<INPUT_PROMPT>(promptFormat: TextGenerationPromptFormat<INPUT_PROMPT, OpenAIChatMessage[]>): PromptFormatTextStreamingModel<INPUT_PROMPT, OpenAIChatMessage[], OpenAIChatSettings, this>;
    withSettings(additionalSettings: Partial<OpenAIChatSettings>): this;
}
declare const openAIChatResponseSchema: z.ZodObject<{
    id: z.ZodString;
    object: z.ZodLiteral<"chat.completion">;
    created: z.ZodNumber;
    model: z.ZodString;
    choices: z.ZodArray<z.ZodObject<{
        message: z.ZodObject<{
            role: z.ZodLiteral<"assistant">;
            content: z.ZodNullable<z.ZodString>;
            function_call: z.ZodOptional<z.ZodObject<{
                name: z.ZodString;
                arguments: z.ZodString;
            }, "strip", z.ZodTypeAny, {
                name: string;
                arguments: string;
            }, {
                name: string;
                arguments: string;
            }>>;
        }, "strip", z.ZodTypeAny, {
            content: string | null;
            role: "assistant";
            function_call?: {
                name: string;
                arguments: string;
            } | undefined;
        }, {
            content: string | null;
            role: "assistant";
            function_call?: {
                name: string;
                arguments: string;
            } | undefined;
        }>;
        index: z.ZodNumber;
        logprobs: z.ZodNullable<z.ZodAny>;
        finish_reason: z.ZodString;
    }, "strip", z.ZodTypeAny, {
        message: {
            content: string | null;
            role: "assistant";
            function_call?: {
                name: string;
                arguments: string;
            } | undefined;
        };
        finish_reason: string;
        index: number;
        logprobs?: any;
    }, {
        message: {
            content: string | null;
            role: "assistant";
            function_call?: {
                name: string;
                arguments: string;
            } | undefined;
        };
        finish_reason: string;
        index: number;
        logprobs?: any;
    }>, "many">;
    usage: z.ZodObject<{
        prompt_tokens: z.ZodNumber;
        completion_tokens: z.ZodNumber;
        total_tokens: z.ZodNumber;
    }, "strip", z.ZodTypeAny, {
        prompt_tokens: number;
        completion_tokens: number;
        total_tokens: number;
    }, {
        prompt_tokens: number;
        completion_tokens: number;
        total_tokens: number;
    }>;
}, "strip", z.ZodTypeAny, {
    object: "chat.completion";
    model: string;
    usage: {
        prompt_tokens: number;
        completion_tokens: number;
        total_tokens: number;
    };
    id: string;
    created: number;
    choices: {
        message: {
            content: string | null;
            role: "assistant";
            function_call?: {
                name: string;
                arguments: string;
            } | undefined;
        };
        finish_reason: string;
        index: number;
        logprobs?: any;
    }[];
}, {
    object: "chat.completion";
    model: string;
    usage: {
        prompt_tokens: number;
        completion_tokens: number;
        total_tokens: number;
    };
    id: string;
    created: number;
    choices: {
        message: {
            content: string | null;
            role: "assistant";
            function_call?: {
                name: string;
                arguments: string;
            } | undefined;
        };
        finish_reason: string;
        index: number;
        logprobs?: any;
    }[];
}>;
export type OpenAIChatResponse = z.infer<typeof openAIChatResponseSchema>;
export type OpenAIChatResponseFormatType<T> = {
    stream: boolean;
    handler: ResponseHandler<T>;
};
export declare const OpenAIChatResponseFormat: {
    /**
     * Returns the response as a JSON object.
     */
    json: {
        stream: false;
        handler: ResponseHandler<{
            object: "chat.completion";
            model: string;
            usage: {
                prompt_tokens: number;
                completion_tokens: number;
                total_tokens: number;
            };
            id: string;
            created: number;
            choices: {
                message: {
                    content: string | null;
                    role: "assistant";
                    function_call?: {
                        name: string;
                        arguments: string;
                    } | undefined;
                };
                finish_reason: string;
                index: number;
                logprobs?: any;
            }[];
        }>;
    };
    /**
     * Returns an async iterable over the text deltas (only the tex different of the first choice).
     */
    textDeltaIterable: {
        stream: true;
        handler: ({ response }: {
            response: Response;
        }) => Promise<AsyncIterable<Delta<string>>>;
    };
    structureDeltaIterable: {
        stream: true;
        handler: ({ response }: {
            response: Response;
        }) => Promise<AsyncIterable<Delta<unknown>>>;
    };
};
export {};
