"use strict";
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.OpenAIChatResponseFormat = exports.OpenAIChatModel = exports.calculateOpenAIChatCostInMillicents = exports.isOpenAIChatModel = exports.getOpenAIChatModelInformation = exports.OPENAI_CHAT_MODELS = void 0;
const secure_json_parse_1 = __importDefault(require("secure-json-parse"));
const zod_1 = require("zod");
const callWithRetryAndThrottle_js_1 = require("../../../core/api/callWithRetryAndThrottle.cjs");
const postToApi_js_1 = require("../../../core/api/postToApi.cjs");
const AbstractModel_js_1 = require("../../../model-function/AbstractModel.cjs");
const StructureParseError_js_1 = require("../../../model-function/generate-structure/StructureParseError.cjs");
const parsePartialJson_js_1 = require("../../../model-function/generate-structure/parsePartialJson.cjs");
const PromptFormatTextStreamingModel_js_1 = require("../../../model-function/generate-text/PromptFormatTextStreamingModel.cjs");
const OpenAIApiConfiguration_js_1 = require("../OpenAIApiConfiguration.cjs");
const OpenAIError_js_1 = require("../OpenAIError.cjs");
const TikTokenTokenizer_js_1 = require("../TikTokenTokenizer.cjs");
const OpenAIChatPromptFormat_js_1 = require("./OpenAIChatPromptFormat.cjs");
const OpenAIChatStreamIterable_js_1 = require("./OpenAIChatStreamIterable.cjs");
const countOpenAIChatMessageTokens_js_1 = require("./countOpenAIChatMessageTokens.cjs");
/*
 * Available OpenAI chat models, their token limits, and pricing.
 *
 * @see https://platform.openai.com/docs/models/
 * @see https://openai.com/pricing
 */
exports.OPENAI_CHAT_MODELS = {
    "gpt-4": {
        contextWindowSize: 8192,
        promptTokenCostInMillicents: 3,
        completionTokenCostInMillicents: 6,
    },
    "gpt-4-0314": {
        contextWindowSize: 8192,
        promptTokenCostInMillicents: 3,
        completionTokenCostInMillicents: 6,
    },
    "gpt-4-0613": {
        contextWindowSize: 8192,
        promptTokenCostInMillicents: 3,
        completionTokenCostInMillicents: 6,
    },
    "gpt-4-32k": {
        contextWindowSize: 32768,
        promptTokenCostInMillicents: 6,
        completionTokenCostInMillicents: 12,
    },
    "gpt-4-32k-0314": {
        contextWindowSize: 32768,
        promptTokenCostInMillicents: 6,
        completionTokenCostInMillicents: 12,
    },
    "gpt-4-32k-0613": {
        contextWindowSize: 32768,
        promptTokenCostInMillicents: 6,
        completionTokenCostInMillicents: 12,
    },
    "gpt-3.5-turbo": {
        contextWindowSize: 4096,
        promptTokenCostInMillicents: 0.15,
        completionTokenCostInMillicents: 0.2,
        fineTunedPromptTokenCostInMillicents: 1.2,
        fineTunedCompletionTokenCostInMillicents: 1.6,
    },
    "gpt-3.5-turbo-0301": {
        contextWindowSize: 4096,
        promptTokenCostInMillicents: 0.15,
        completionTokenCostInMillicents: 0.2,
    },
    "gpt-3.5-turbo-0613": {
        contextWindowSize: 4096,
        promptTokenCostInMillicents: 0.15,
        completionTokenCostInMillicents: 0.2,
        fineTunedPromptTokenCostInMillicents: 1.2,
        fineTunedCompletionTokenCostInMillicents: 1.6,
    },
    "gpt-3.5-turbo-16k": {
        contextWindowSize: 16384,
        promptTokenCostInMillicents: 0.3,
        completionTokenCostInMillicents: 0.4,
    },
    "gpt-3.5-turbo-16k-0613": {
        contextWindowSize: 16384,
        promptTokenCostInMillicents: 0.3,
        completionTokenCostInMillicents: 0.4,
    },
};
function getOpenAIChatModelInformation(model) {
    // Model is already a base model:
    if (model in exports.OPENAI_CHAT_MODELS) {
        const baseModelInformation = exports.OPENAI_CHAT_MODELS[model];
        return {
            baseModel: model,
            isFineTuned: false,
            contextWindowSize: baseModelInformation.contextWindowSize,
            promptTokenCostInMillicents: baseModelInformation.promptTokenCostInMillicents,
            completionTokenCostInMillicents: baseModelInformation.completionTokenCostInMillicents,
        };
    }
    // Extract the base model from the fine-tuned model:
    // eslint-disable-next-line @typescript-eslint/no-unused-vars
    const [_, baseModel, ___, ____, _____] = model.split(":");
    if (["gpt-3.5-turbo", "gpt-3.5-turbo-0613"].includes(baseModel)) {
        const baseModelInformation = exports.OPENAI_CHAT_MODELS[baseModel];
        return {
            baseModel: baseModel,
            isFineTuned: true,
            contextWindowSize: baseModelInformation.contextWindowSize,
            promptTokenCostInMillicents: baseModelInformation.fineTunedPromptTokenCostInMillicents,
            completionTokenCostInMillicents: baseModelInformation.fineTunedCompletionTokenCostInMillicents,
        };
    }
    throw new Error(`Unknown OpenAI chat base model ${baseModel}.`);
}
exports.getOpenAIChatModelInformation = getOpenAIChatModelInformation;
const isOpenAIChatModel = (model) => model in exports.OPENAI_CHAT_MODELS ||
    model.startsWith("ft:gpt-3.5-turbo-0613:") ||
    model.startsWith("ft:gpt-3.5-turbo:");
exports.isOpenAIChatModel = isOpenAIChatModel;
const calculateOpenAIChatCostInMillicents = ({ model, response, }) => {
    const modelInformation = getOpenAIChatModelInformation(model);
    return (response.usage.prompt_tokens *
        modelInformation.promptTokenCostInMillicents +
        response.usage.completion_tokens *
            modelInformation.completionTokenCostInMillicents);
};
exports.calculateOpenAIChatCostInMillicents = calculateOpenAIChatCostInMillicents;
/**
 * Create a text generation model that calls the OpenAI chat completion API.
 *
 * @see https://platform.openai.com/docs/api-reference/chat/create
 *
 * @example
 * const model = new OpenAIChatModel({
 *   model: "gpt-3.5-turbo",
 *   temperature: 0.7,
 *   maxCompletionTokens: 500,
 * });
 *
 * const text = await generateText([
 *   model,
 *   OpenAIChatMessage.system(
 *     "Write a short story about a robot learning to love:"
 *   ),
 * ]);
 */
class OpenAIChatModel extends AbstractModel_js_1.AbstractModel {
    constructor(settings) {
        super({ settings });
        Object.defineProperty(this, "provider", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "openai"
        });
        Object.defineProperty(this, "contextWindowSize", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "tokenizer", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        const modelInformation = getOpenAIChatModelInformation(this.settings.model);
        this.tokenizer = new TikTokenTokenizer_js_1.TikTokenTokenizer({
            model: modelInformation.baseModel,
        });
        this.contextWindowSize = modelInformation.contextWindowSize;
    }
    get modelName() {
        return this.settings.model;
    }
    /**
     * Counts the prompt tokens required for the messages. This includes the message base tokens
     * and the prompt base tokens.
     */
    countPromptTokens(messages) {
        return (0, countOpenAIChatMessageTokens_js_1.countOpenAIChatPromptTokens)({
            messages,
            model: this.modelName,
        });
    }
    async callAPI(messages, options) {
        return (0, callWithRetryAndThrottle_js_1.callWithRetryAndThrottle)({
            retry: this.settings.api?.retry,
            throttle: this.settings.api?.throttle,
            call: async () => callOpenAIChatCompletionAPI({
                ...this.settings,
                // function calling:
                functions: options.functions ?? this.settings.functions,
                functionCall: options.functionCall ?? this.settings.functionCall,
                // map to OpenAI API names:
                stop: this.settings.stopSequences,
                maxTokens: this.settings.maxCompletionTokens,
                // other settings:
                user: this.settings.isUserIdForwardingEnabled
                    ? options.run?.userId
                    : undefined,
                abortSignal: options.run?.abortSignal,
                responseFormat: options.responseFormat,
                messages,
            }),
        });
    }
    get settingsForEvent() {
        const eventSettingProperties = [
            "stopSequences",
            "maxCompletionTokens",
            "functions",
            "functionCall",
            "temperature",
            "topP",
            "n",
            "presencePenalty",
            "frequencyPenalty",
            "logitBias",
        ];
        return Object.fromEntries(Object.entries(this.settings).filter(([key]) => eventSettingProperties.includes(key)));
    }
    async doGenerateText(prompt, options) {
        const response = await this.callAPI(prompt, {
            ...options,
            responseFormat: exports.OpenAIChatResponseFormat.json,
        });
        return {
            response,
            text: response.choices[0].message.content,
            usage: this.extractUsage(response),
        };
    }
    doStreamText(prompt, options) {
        return this.callAPI(prompt, {
            ...options,
            responseFormat: exports.OpenAIChatResponseFormat.textDeltaIterable,
        });
    }
    /**
     * JSON generation uses the OpenAI GPT function calling API.
     * It provides a single function specification and instructs the model to provide parameters for calling the function.
     * The result is returned as parsed JSON.
     *
     * @see https://platform.openai.com/docs/guides/gpt/function-calling
     */
    async doGenerateStructure(structureDefinition, prompt, options) {
        const response = await this.callAPI(prompt, {
            ...options,
            responseFormat: exports.OpenAIChatResponseFormat.json,
            functionCall: { name: structureDefinition.name },
            functions: [
                {
                    name: structureDefinition.name,
                    description: structureDefinition.description,
                    parameters: structureDefinition.schema.getJsonSchema(),
                },
            ],
        });
        const valueText = response.choices[0].message.function_call.arguments;
        try {
            return {
                response,
                valueText,
                value: secure_json_parse_1.default.parse(valueText),
                usage: this.extractUsage(response),
            };
        }
        catch (error) {
            throw new StructureParseError_js_1.StructureParseError({
                structureName: structureDefinition.name,
                valueText,
                cause: error,
            });
        }
    }
    async doStreamStructure(structureDefinition, prompt, options) {
        return this.callAPI(prompt, {
            ...options,
            responseFormat: exports.OpenAIChatResponseFormat.structureDeltaIterable,
            functionCall: { name: structureDefinition.name },
            functions: [
                {
                    name: structureDefinition.name,
                    description: structureDefinition.description,
                    parameters: structureDefinition.schema.getJsonSchema(),
                },
            ],
        });
    }
    async doGenerateStructureOrText(structureDefinitions, prompt, options) {
        const response = await this.callAPI(prompt, {
            ...options,
            responseFormat: exports.OpenAIChatResponseFormat.json,
            functionCall: "auto",
            functions: structureDefinitions.map((structureDefinition) => ({
                name: structureDefinition.name,
                description: structureDefinition.description,
                parameters: structureDefinition.schema.getJsonSchema(),
            })),
        });
        const message = response.choices[0].message;
        const content = message.content;
        const functionCall = message.function_call;
        if (functionCall == null) {
            return {
                response,
                structureAndText: {
                    structure: null,
                    value: null,
                    valueText: null,
                    text: content ?? "",
                },
                usage: this.extractUsage(response),
            };
        }
        try {
            return {
                response,
                structureAndText: {
                    structure: functionCall.name,
                    value: secure_json_parse_1.default.parse(functionCall.arguments),
                    valueText: functionCall.arguments,
                    text: content,
                },
                usage: this.extractUsage(response),
            };
        }
        catch (error) {
            throw new StructureParseError_js_1.StructureParseError({
                structureName: functionCall.name,
                valueText: functionCall.arguments,
                cause: error,
            });
        }
    }
    extractUsage(response) {
        return {
            promptTokens: response.usage.prompt_tokens,
            completionTokens: response.usage.completion_tokens,
            totalTokens: response.usage.total_tokens,
        };
    }
    /**
     * Returns this model with an instruction prompt format.
     */
    withInstructionPrompt() {
        return this.withPromptFormat((0, OpenAIChatPromptFormat_js_1.mapInstructionPromptToOpenAIChatFormat)());
    }
    /**
     * Returns this model with a chat prompt format.
     */
    withChatPrompt() {
        return this.withPromptFormat((0, OpenAIChatPromptFormat_js_1.mapChatPromptToOpenAIChatFormat)());
    }
    withPromptFormat(promptFormat) {
        return new PromptFormatTextStreamingModel_js_1.PromptFormatTextStreamingModel({
            model: this.withSettings({
                stopSequences: [
                    ...(this.settings.stopSequences ?? []),
                    ...promptFormat.stopSequences,
                ],
            }),
            promptFormat,
        });
    }
    withSettings(additionalSettings) {
        return new OpenAIChatModel(Object.assign({}, this.settings, additionalSettings));
    }
}
exports.OpenAIChatModel = OpenAIChatModel;
const openAIChatResponseSchema = zod_1.z.object({
    id: zod_1.z.string(),
    object: zod_1.z.literal("chat.completion"),
    created: zod_1.z.number(),
    model: zod_1.z.string(),
    choices: zod_1.z.array(zod_1.z.object({
        message: zod_1.z.object({
            role: zod_1.z.literal("assistant"),
            content: zod_1.z.string().nullable(),
            function_call: zod_1.z
                .object({
                name: zod_1.z.string(),
                arguments: zod_1.z.string(),
            })
                .optional(),
        }),
        index: zod_1.z.number(),
        logprobs: zod_1.z.nullable(zod_1.z.any()),
        finish_reason: zod_1.z.string(),
    })),
    usage: zod_1.z.object({
        prompt_tokens: zod_1.z.number(),
        completion_tokens: zod_1.z.number(),
        total_tokens: zod_1.z.number(),
    }),
});
async function callOpenAIChatCompletionAPI({ api = new OpenAIApiConfiguration_js_1.OpenAIApiConfiguration(), abortSignal, responseFormat, model, messages, functions, functionCall, temperature, topP, n, stop, maxTokens, presencePenalty, frequencyPenalty, logitBias, user, }) {
    // empty arrays are not allowed for stop:
    if (stop != null && Array.isArray(stop) && stop.length === 0) {
        stop = undefined;
    }
    return (0, postToApi_js_1.postJsonToApi)({
        url: api.assembleUrl("/chat/completions"),
        headers: api.headers,
        body: {
            stream: responseFormat.stream,
            model,
            messages,
            functions,
            function_call: functionCall,
            temperature,
            top_p: topP,
            n,
            stop,
            max_tokens: maxTokens,
            presence_penalty: presencePenalty,
            frequency_penalty: frequencyPenalty,
            logit_bias: logitBias,
            user,
        },
        failedResponseHandler: OpenAIError_js_1.failedOpenAICallResponseHandler,
        successfulResponseHandler: responseFormat.handler,
        abortSignal,
    });
}
exports.OpenAIChatResponseFormat = {
    /**
     * Returns the response as a JSON object.
     */
    json: {
        stream: false,
        handler: (0, postToApi_js_1.createJsonResponseHandler)(openAIChatResponseSchema),
    },
    /**
     * Returns an async iterable over the text deltas (only the tex different of the first choice).
     */
    textDeltaIterable: {
        stream: true,
        handler: async ({ response }) => (0, OpenAIChatStreamIterable_js_1.createOpenAIChatDeltaIterableQueue)(response.body, (delta) => delta[0]?.delta.content ?? ""),
    },
    structureDeltaIterable: {
        stream: true,
        handler: async ({ response }) => (0, OpenAIChatStreamIterable_js_1.createOpenAIChatDeltaIterableQueue)(response.body, (delta) => (0, parsePartialJson_js_1.parsePartialJson)(delta[0]?.function_call?.arguments)),
    },
};
