"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.AnthropicTextGenerationResponseFormat = exports.AnthropicTextGenerationModel = exports.ANTHROPIC_TEXT_GENERATION_MODELS = void 0;
const zod_1 = require("zod");
const callWithRetryAndThrottle_js_1 = require("../../core/api/callWithRetryAndThrottle.cjs");
const postToApi_js_1 = require("../../core/api/postToApi.cjs");
const AsyncQueue_js_1 = require("../../event-source/AsyncQueue.cjs");
const parseEventSourceStream_js_1 = require("../../event-source/parseEventSourceStream.cjs");
const AbstractModel_js_1 = require("../../model-function/AbstractModel.cjs");
const PromptFormatTextStreamingModel_js_1 = require("../../model-function/generate-text/PromptFormatTextStreamingModel.cjs");
const parseJSON_js_1 = require("../../util/parseJSON.cjs");
const AnthropicApiConfiguration_js_1 = require("./AnthropicApiConfiguration.cjs");
const AnthropicError_js_1 = require("./AnthropicError.cjs");
const AnthropicPromptFormat_js_1 = require("./AnthropicPromptFormat.cjs");
exports.ANTHROPIC_TEXT_GENERATION_MODELS = {
    "claude-instant-1": {
        contextWindowSize: 100000,
    },
    "claude-instant-1.2": {
        contextWindowSize: 100000,
    },
    "claude-2": {
        contextWindowSize: 100000,
    },
    "claude-2.0": {
        contextWindowSize: 100000,
    },
};
/**
 * Create a text generation model that calls the Anthropic API.
 *
 * @see https://docs.anthropic.com/claude/reference/complete_post
 */
class AnthropicTextGenerationModel extends AbstractModel_js_1.AbstractModel {
    constructor(settings) {
        super({ settings });
        Object.defineProperty(this, "provider", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: "anthropic"
        });
        Object.defineProperty(this, "contextWindowSize", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: void 0
        });
        Object.defineProperty(this, "tokenizer", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: undefined
        });
        Object.defineProperty(this, "countPromptTokens", {
            enumerable: true,
            configurable: true,
            writable: true,
            value: undefined
        });
        this.contextWindowSize =
            exports.ANTHROPIC_TEXT_GENERATION_MODELS[this.settings.model].contextWindowSize;
    }
    get modelName() {
        return this.settings.model;
    }
    async callAPI(prompt, options) {
        return (0, callWithRetryAndThrottle_js_1.callWithRetryAndThrottle)({
            retry: this.settings.api?.retry,
            throttle: this.settings.api?.throttle,
            call: async () => callAnthropicTextGenerationAPI({
                ...this.settings,
                stopSequences: this.settings.stopSequences,
                maxTokens: this.settings.maxCompletionTokens,
                abortSignal: options.run?.abortSignal,
                responseFormat: options.responseFormat,
                prompt,
            }),
        });
    }
    get settingsForEvent() {
        const eventSettingProperties = [
            "maxCompletionTokens",
            "stopSequences",
            "temperature",
            "topK",
            "topP",
            "userId",
        ];
        return Object.fromEntries(Object.entries(this.settings).filter(([key]) => eventSettingProperties.includes(key)));
    }
    async doGenerateText(prompt, options) {
        const response = await this.callAPI(prompt, {
            ...options,
            responseFormat: exports.AnthropicTextGenerationResponseFormat.json,
        });
        return {
            response,
            text: response.completion,
        };
    }
    doStreamText(prompt, options) {
        return this.callAPI(prompt, {
            ...options,
            responseFormat: exports.AnthropicTextGenerationResponseFormat.deltaIterable,
        });
    }
    /**
     * Returns this model with an instruction prompt format.
     */
    withInstructionPrompt() {
        return this.withPromptFormat((0, AnthropicPromptFormat_js_1.mapInstructionPromptToAnthropicFormat)());
    }
    /**
     * Returns this model with a chat prompt format.
     */
    withChatPrompt() {
        return this.withPromptFormat((0, AnthropicPromptFormat_js_1.mapChatPromptToAnthropicFormat)());
    }
    withPromptFormat(promptFormat) {
        return new PromptFormatTextStreamingModel_js_1.PromptFormatTextStreamingModel({
            model: this.withSettings({
                stopSequences: [
                    ...(this.settings.stopSequences ?? []),
                    ...promptFormat.stopSequences,
                ],
            }),
            promptFormat,
        });
    }
    withSettings(additionalSettings) {
        return new AnthropicTextGenerationModel(Object.assign({}, this.settings, additionalSettings));
    }
}
exports.AnthropicTextGenerationModel = AnthropicTextGenerationModel;
const anthropicTextGenerationResponseSchema = zod_1.z.object({
    completion: zod_1.z.string(),
    stop_reason: zod_1.z.string(),
    model: zod_1.z.string(),
});
async function callAnthropicTextGenerationAPI({ api = new AnthropicApiConfiguration_js_1.AnthropicApiConfiguration(), abortSignal, responseFormat, model, prompt, maxTokens, stopSequences, temperature, topK, topP, userId, }) {
    return (0, postToApi_js_1.postJsonToApi)({
        url: api.assembleUrl(`/complete`),
        headers: api.headers,
        body: {
            model,
            prompt,
            stream: responseFormat.stream,
            max_tokens_to_sample: maxTokens,
            temperature,
            top_k: topK,
            top_p: topP,
            stop_sequences: stopSequences,
            metadata: userId != null ? { user_id: userId } : undefined,
        },
        failedResponseHandler: AnthropicError_js_1.failedAnthropicCallResponseHandler,
        successfulResponseHandler: responseFormat.handler,
        abortSignal,
    });
}
const anthropicTextStreamingResponseSchema = zod_1.z.object({
    completion: zod_1.z.string(),
    stop_reason: zod_1.z.string().nullable(),
    model: zod_1.z.string(),
});
async function createAnthropicFullDeltaIterableQueue(stream) {
    const queue = new AsyncQueue_js_1.AsyncQueue();
    let content = "";
    // process the stream asynchonously (no 'await' on purpose):
    (0, parseEventSourceStream_js_1.parseEventSourceStream)({ stream })
        .then(async (events) => {
        try {
            for await (const event of events) {
                if (event.event === "error") {
                    queue.push({ type: "error", error: event.data });
                    queue.close();
                    return;
                }
                if (event.event !== "completion") {
                    continue;
                }
                const data = event.data;
                const eventData = (0, parseJSON_js_1.parseJsonWithZod)(data, anthropicTextStreamingResponseSchema);
                content += eventData.completion;
                queue.push({
                    type: "delta",
                    fullDelta: {
                        content,
                        isComplete: eventData.stop_reason != null,
                        delta: eventData.completion,
                    },
                    valueDelta: eventData.completion,
                });
                if (eventData.stop_reason != null) {
                    queue.close();
                }
            }
        }
        catch (error) {
            queue.push({ type: "error", error });
            queue.close();
        }
    })
        .catch((error) => {
        queue.push({ type: "error", error });
        queue.close();
    });
    return queue;
}
exports.AnthropicTextGenerationResponseFormat = {
    /**
     * Returns the response as a JSON object.
     */
    json: {
        stream: false,
        handler: (0, postToApi_js_1.createJsonResponseHandler)(anthropicTextGenerationResponseSchema),
    },
    /**
     * Returns an async iterable over the full deltas (all choices, including full current state at time of event)
     * of the response stream.
     */
    deltaIterable: {
        stream: true,
        handler: async ({ response }) => createAnthropicFullDeltaIterableQueue(response.body),
    },
};
